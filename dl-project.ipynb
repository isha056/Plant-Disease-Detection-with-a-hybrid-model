{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":182633,"sourceType":"datasetVersion","datasetId":78313}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Core Libraries\nimport os\nimport sys\nimport torch\nimport torchvision\nimport numpy as np\nimport random\nimport cv2\nfrom PIL import Image\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\nimport csv\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n\n# Deep Learning Frameworks\nfrom transformers import AutoModel, AutoConfig, AutoImageProcessor, SegformerForSemanticSegmentation\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\n# Image Processing & Augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom skimage import measure\n\nprint(\"✓ All libraries imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:38.159893Z","iopub.execute_input":"2025-11-05T04:10:38.160179Z","iopub.status.idle":"2025-11-05T04:10:49.098701Z","shell.execute_reply.started":"2025-11-05T04:10:38.160153Z","shell.execute_reply":"2025-11-05T04:10:49.097929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set Deterministic Seeds for Reproducibility\nSEED = 42\nTIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\ndef set_seed(seed=42):\n    \"\"\"Set seeds for reproducibility across Python, NumPy, and PyTorch.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(SEED)\n\nprint(f\"✓ Deterministic seed set: {SEED}\")\nprint(f\"✓ Timestamp: {TIMESTAMP}\")\nprint(f\"✓ Reproducibility ensured across Python, NumPy, and PyTorch.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.100414Z","iopub.execute_input":"2025-11-05T04:10:49.100639Z","iopub.status.idle":"2025-11-05T04:10:49.108479Z","shell.execute_reply.started":"2025-11-05T04:10:49.100623Z","shell.execute_reply":"2025-11-05T04:10:49.107814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Output Directories\nOUTPUT_DIRS = {\n    'figures': 'figures',\n    'tables': 'tables',\n    'models': 'models',\n    'logs': 'logs',\n    'data': 'data'\n}\n\nfor dir_name, dir_path in OUTPUT_DIRS.items():\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n    print(f\"✓ Created directory: {dir_path}/\")\n\nprint(\"\\n✓ All output directories ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.109164Z","iopub.execute_input":"2025-11-05T04:10:49.109442Z","iopub.status.idle":"2025-11-05T04:10:49.122156Z","shell.execute_reply.started":"2025-11-05T04:10:49.109419Z","shell.execute_reply":"2025-11-05T04:10:49.121346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PlantVillage Class Names (38 classes)\nPLANTVILLAGE_CLASSES = [\n    \"Apple___Apple_scab\", \"Apple___Black_rot\", \"Apple___Cedar_apple_rust\", \"Apple___healthy\",\n    \"Blueberry___healthy\", \"Cherry_(including_sour)___Powdery_mildew\", \"Cherry_(including_sour)___healthy\",\n    \"Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\", \"Corn_(maize)___Common_rust_\",\n    \"Corn_(maize)___Northern_Leaf_Blight\", \"Corn_(maize)___healthy\", \"Grape___Black_rot\",\n    \"Grape___Esca_(Black_Measles)\", \"Grape___healthy\", \"Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\",\n    \"Orange___Haunglongbing_(Citrus_greening)\", \"Peach___Bacterial_spot\", \"Peach___healthy\",\n    \"Pepper,_bell___Bacterial_spot\", \"Pepper,_bell___healthy\", \"Potato___Early_blight\",\n    \"Potato___Late_blight\", \"Potato___healthy\", \"Raspberry___healthy\", \"Soybean___healthy\",\n    \"Squash___Powdery_mildew\", \"Strawberry___Leaf_scorch\", \"Strawberry___healthy\",\n    \"Tomato___Bacterial_spot\", \"Tomato___Early_blight\", \"Tomato___Late_blight\",\n    \"Tomato___Leaf_Mold\", \"Tomato___Septoria_leaf_spot\",\n    \"Tomato___Spider_mites Two-spotted_spider_mite\", \"Tomato___Target_Spot\",\n    \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\", \"Tomato___Tomato_mosaic_virus\", \"Tomato___healthy\"\n]\n\nNUM_CLASSES = len(PLANTVILLAGE_CLASSES)\nprint(f\"✓ PlantVillage dataset: {NUM_CLASSES} classes defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.122936Z","iopub.execute_input":"2025-11-05T04:10:49.123195Z","iopub.status.idle":"2025-11-05T04:10:49.138261Z","shell.execute_reply.started":"2025-11-05T04:10:49.12317Z","shell.execute_reply":"2025-11-05T04:10:49.137444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration (These are the missing definitions) ---\n# Set to True if you are using a custom dataset path locally\nUSE_CUSTOM_DATA = False \n# Set to True to run a quick test with synthetic data (which we'll need to add)\nUSE_SMOKE_TEST = False \n# Default local path if not on Kaggle and not a smoke test\nDEFAULT_DATA_PATH = \"data\" \n# -----------------------------------------------------\n\nIN_KAGGLE = os.path.exists('/kaggle/input')\nKAGGLE_INPUT_PATH = None\n\nif IN_KAGGLE:\n    print(\"✓ Kaggle environment detected!\")\n    # Search for PlantVillage dataset in /kaggle/input/\n    kaggle_datasets = [d for d in Path('/kaggle/input').iterdir() if d.is_dir()]\n    print(f\"  Found {len(kaggle_datasets)} input dataset(s):\")\n    for dataset in kaggle_datasets:\n        print(f\"    - {dataset.name}\")\n    \n    # Try to find PlantVillage dataset\n    for dataset in kaggle_datasets:\n        # Check common names for PlantVillage dataset\n        if any(keyword in dataset.name.lower() for keyword in ['plant', 'disease', 'village']):\n            KAGGLE_INPUT_PATH = str(dataset)\n            print(f\"\\n  ✓ Auto-detected PlantVillage dataset: {dataset.name}\")\n            print(f\"    Path: {KAGGLE_INPUT_PATH}\")\n            break\n\n# -----------------------------------------------------------------\n# ▼▼▼ LOGIC BRIDGE (This part is correct) ▼▼▼\n# -----------------------------------------------------------------\n# Determine the final DATASET_ROOT based on the environment\nif IN_KAGGLE and KAGGLE_INPUT_PATH:\n    DATASET_ROOT = KAGGLE_INPUT_PATH\n    DATASET_ROOT = str(Path(DATASET_ROOT) / 'new plant diseases dataset(augmented)')\n    DATASET_ROOT = str(Path(DATASET_ROOT) / 'New Plant Diseases Dataset(Augmented)')\nelif USE_SMOKE_TEST:\n    DATASET_ROOT = \"data\" # We will create smoke test data in 'data/'\nelse:\n    # Default case: Standard data expected in 'data'\n    DATASET_ROOT = DEFAULT_DATA_PATH \n\nprint(f\"\\n► Using dataset root: {DATASET_ROOT}\")\n# -----------------------------------------------------------------\n# ▲▲▲ END OF LOGIC BRIDGE ▲▲▲\n# -----------------------------------------------------------------\n\n# -----------------------------------------------------------------\n# ▼▼▼ NEW SCANNING LOGIC TO HANDLE TRAIN/VALID FOLDERS ▼▼▼\n# -----------------------------------------------------------------\ndataset_path = Path(DATASET_ROOT)\n\nall_image_paths = []\nall_labels = []\nclass_image_counts = {}\n\n# Define the subfolders to scan (this dataset has a train/valid split)\nsplit_folders = [dataset_path / 'train', dataset_path / 'valid']\n\n# Check if these folders exist\nif not split_folders[0].exists() or not split_folders[1].exists():\n    print(f\"❌ ERROR: Expected 'train' and 'valid' folders in {DATASET_ROOT}\")\n    print(\"  This dataset appears to be pre-split. Looking for 'train' and 'valid' dirs.\")\n    raise RuntimeError(\"Invalid dataset structure. 'train' or 'valid' missing.\")\n\n# Discover classes from the 'train' directory (assuming both have the same classes)\ntry:\n    discovered_classes = sorted([d.name for d in split_folders[0].iterdir() if d.is_dir()])\nexcept FileNotFoundError:\n    print(f\"❌ ERROR: Dataset directory not found: {split_folders[0]}\")\n    raise\n\nif len(discovered_classes) == 0:\n    print(f\"❌ ERROR: No class folders (e.g., 'Apple___scab') found inside {split_folders[0]}\")\n    raise RuntimeError(\"Invalid dataset structure. 'train' folder is empty or has wrong structure.\")\n\n# Create a mapping from discovered class name to its index\nclass_to_idx = {class_name: idx for idx, class_name in enumerate(discovered_classes)}\nprint(f\"✓ Discovered {len(discovered_classes)} classes from {split_folders[0]}.\")\n\n# Loop over both 'train' and 'valid' to collect ALL images\nfor split_dir in split_folders:\n    print(f\"  Scanning {split_dir.name}...\")\n    for class_name in discovered_classes:\n        class_dir = split_dir / class_name\n        if not class_dir.is_dir():\n            # It's possible 'valid' might not have all 38 classes, so just warn\n            print(f\"  - Warning: Class '{class_name}' not found in '{split_dir.name}', skipping.\")\n            continue\n            \n        image_files = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.jpeg\")) + \\\n                      list(class_dir.glob(\"*.png\")) + list(class_dir.glob(\"*.JPG\"))\n        \n        # Add to total counts\n        if class_name not in class_image_counts:\n            class_image_counts[class_name] = 0\n        class_image_counts[class_name] += len(image_files)\n        \n        # Add to master lists\n        for img_path in image_files:\n            all_image_paths.append(str(img_path))\n            all_labels.append(class_to_idx[class_name])\n\n# -----------------------------------------------------------------\n# ▲▲▲ END OF NEW SCANNING LOGIC ▲▲▲\n# -----------------------------------------------------------------\n\n# Update classes if using custom dataset\nif USE_CUSTOM_DATA or USE_SMOKE_TEST or IN_KAGGLE:\n    PLANTVILLAGE_CLASSES = discovered_classes\n    NUM_CLASSES = len(PLANTVILLAGE_CLASSES)\n\nprint(f\"\\n✓ Dataset root: {DATASET_ROOT}\")\nprint(f\"✓ Number of classes: {NUM_CLASSES}\")\nprint(f\"✓ Total images found (from train + valid): {len(all_image_paths)}\")\nprint(f\"\\nClass distribution (Combined):\")\nfor class_name, count in list(class_image_counts.items())[:10]:\n    print(f\"  {class_name}: {count} images\")\nif len(class_image_counts) > 10:\n    print(f\"  ... and {len(class_image_counts) - 10} more classes\")\n\nif len(all_image_paths) == 0:\n     raise RuntimeError(\"Failed to find any images after scanning. Check paths again.\")\nelse:\n    print(f\"\\n✓ Dataset validation passed! Found {len(all_image_paths)} images.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.139118Z","iopub.execute_input":"2025-11-05T04:10:49.139963Z","iopub.status.idle":"2025-11-05T04:10:49.736626Z","shell.execute_reply.started":"2025-11-05T04:10:49.139945Z","shell.execute_reply":"2025-11-05T04:10:49.735816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Deterministic Train/Val/Test Split\nfrom sklearn.model_selection import train_test_split\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING TRAIN/VAL/TEST SPLIT\")\nprint(\"=\"*60)\n\n# Split: 70% train, 15% val, 15% test\ntrain_paths, temp_paths, train_labels, temp_labels = train_test_split(\n    all_image_paths, all_labels, test_size=0.3, random_state=SEED, stratify=all_labels\n)\n\nval_paths, test_paths, val_labels, test_labels = train_test_split(\n    temp_paths, temp_labels, test_size=0.5, random_state=SEED, stratify=temp_labels\n)\n\nprint(f\"Train set: {len(train_paths)} images ({len(train_paths)/len(all_image_paths)*100:.1f}%)\")\nprint(f\"Val set:   {len(val_paths)} images ({len(val_paths)/len(all_image_paths)*100:.1f}%)\")\nprint(f\"Test set:  {len(test_paths)} images ({len(test_paths)/len(all_image_paths)*100:.1f}%)\")\n\n# Verify stratification\ntrain_class_dist = pd.Series(train_labels).value_counts().sort_index()\nval_class_dist = pd.Series(val_labels).value_counts().sort_index()\ntest_class_dist = pd.Series(test_labels).value_counts().sort_index()\n\nprint(f\"\\n✓ Split is stratified (each class proportionally represented)\")\nprint(f\"✓ Random seed: {SEED} (reproducible)\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.73741Z","iopub.execute_input":"2025-11-05T04:10:49.737632Z","iopub.status.idle":"2025-11-05T04:10:49.853146Z","shell.execute_reply.started":"2025-11-05T04:10:49.737614Z","shell.execute_reply":"2025-11-05T04:10:49.852451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Environment Setup (GPU/CPU)\nimport torch\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"=\"*60)\nprint(\"ENVIRONMENT SETUP\")\nprint(\"=\"*60)\nprint(f\"✓ Using device: {DEVICE}\")\n\nif DEVICE.type == 'cuda':\n    print(f\"✓ GPU Name: {torch.cuda.get_device_name(0)}\")\n    \nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.853926Z","iopub.execute_input":"2025-11-05T04:10:49.854198Z","iopub.status.idle":"2025-11-05T04:10:49.859679Z","shell.execute_reply.started":"2025-11-05T04:10:49.854178Z","shell.execute_reply":"2025-11-05T04:10:49.858874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Pre-trained MAE Model (Proxy for Self-Supervised Pretraining)\nprint(\"=\"*60)\nprint(\"LOADING MASKED AUTOENCODER (MAE) MODEL\")\nprint(\"=\"*60)\n\n# For reproducibility, we use a pre-trained MAE model as a proxy for the pretraining stage\n# In production, this would be replaced with actual MAE pretraining on unlabeled plant images\n\ntry:\n    # Load ViT-Base/16 pretrained with MAE\n    from transformers import ViTMAEForPreTraining, ViTMAEConfig, AutoImageProcessor\n    \n    print(\"Loading MAE model (ViT-Base/16 architecture)...\")\n    print(\"Model: facebook/vit-mae-base\")\n    \n    mae_config = ViTMAEConfig.from_pretrained(\"facebook/vit-mae-base\")\n    mae_model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n    \n    print(f\"✓ MAE model loaded successfully\")\n    print(f\"  Architecture: ViT-Base/16\")\n    print(f\"  Hidden size: {mae_config.hidden_size}\")\n    print(f\"  Number of layers: {mae_config.num_hidden_layers}\")\n    print(f\"  Number of attention heads: {mae_config.num_attention_heads}\")\n    print(f\"  Patch size: {mae_config.patch_size}×{mae_config.patch_size}\")\n    print(f\"  Image size: {mae_config.image_size}×{mae_config.image_size}\")\n    \n    # Extract encoder for downstream tasks\n    mae_encoder = mae_model.vit\n    print(f\"✓ MAE encoder extracted for feature extraction\")\n    \n    # Load image processor\n    mae_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n    print(f\"✓ Image processor loaded\")\n    \n    MAE_LOADED = True\n    \nexcept Exception as e:\n    print(f\"⚠️  Warning: Could not load MAE model: {e}\")\n    print(\"   Falling back to standard ViT-Base/16 pretrained on ImageNet\")\n    \n    from transformers import ViTModel, ViTConfig, AutoImageProcessor\n    mae_config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n    mae_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n    mae_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n    print(f\"✓ Fallback ViT-Base/16 loaded\")\n    \n    MAE_LOADED = False\n\n# Move to device\nmae_encoder.to(DEVICE)\nmae_encoder.eval()\nprint(f\"✓ Model moved to: {DEVICE}\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:49.863019Z","iopub.execute_input":"2025-11-05T04:10:49.863239Z","iopub.status.idle":"2025-11-05T04:10:51.607593Z","shell.execute_reply.started":"2025-11-05T04:10:49.863222Z","shell.execute_reply":"2025-11-05T04:10:51.606612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Demonstrate MAE Masking Strategy\nprint(\"\\n\" + \"=\"*60)\nprint(\"MAE MASKING DEMONSTRATION\")\nprint(\"=\"*60)\n\ntry:\n    # --- FIX 1: Use 'train_paths' (what we defined earlier) ---\n    sample_img_path = train_paths[0]\n    # --------------------------------------------------------\n    \n    sample_img = Image.open(sample_img_path).convert('RGB').resize((224, 224))\n    sample_img_array = np.array(sample_img)\n\n    print(f\"Sample image: {Path(sample_img_path).name}\")\n    print(f\"Image shape: {sample_img_array.shape}\")\n\n    # MAE uses 75% masking ratio\n    MASK_RATIO = 0.75\n    num_patches = (224 // 16) ** 2  # 14x14 = 196 patches for patch_size=16\n    num_masked = int(num_patches * MASK_RATIO)\n\n    print(f\"\\nMAE Masking Parameters:\")\n    print(f\"  Total patches: {num_patches} (14x14 grid)\")\n    print(f\"  Mask ratio: {MASK_RATIO} ({MASK_RATIO*100:.0f}%)\")\n    print(f\"  Masked patches: {num_masked}\")\n    print(f\"  Visible patches: {num_patches - num_masked}\")\n\n    # Process image\n    inputs = mae_processor(images=sample_img, return_tensors=\"pt\")\n    \n    # Generate random mask\n    torch.manual_seed(SEED)\n    noise = torch.rand(1, num_patches)\n    ids_shuffle = torch.argsort(noise, dim=1)\n    len_keep = num_patches - num_masked\n    ids_keep = ids_shuffle[:, :len_keep]\n    \n    # Create visualization mask\n    mask_visual = torch.ones(num_patches)\n    mask_visual[ids_keep] = 0\n    mask_visual = mask_visual.reshape(14, 14).numpy()\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axes[0].imshow(sample_img_array)\n    axes[0].set_title(\"Original Image\", fontsize=12, fontweight='bold')\n    axes[0].axis('off')\n    \n    axes[1].imshow(mask_visual, cmap='RdYlGn_r', vmin=0, vmax=1)\n    axes[1].set_title(f\"Masking Pattern\\n(75% masked = red)\", fontsize=12, fontweight='bold')\n    axes[1].axis('off')\n    \n    # Overlay mask on image\n    mask_overlay = cv2.resize(mask_visual, (224, 224), interpolation=cv2.INTER_NEAREST)\n    masked_img = sample_img_array.copy()\n    masked_img[mask_overlay > 0.5] = [128, 128, 128]  # Gray out masked patches\n    axes[2].imshow(masked_img)\n    axes[2].set_title(\"Masked Image View\\n(gray = masked regions)\", fontsize=12, fontweight='bold')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('figures/mae_masking_demo.png', dpi=300, bbox_inches='tight')\n    plt.close() # Use plt.close() to save without displaying in the log\n    \n    print(f\"\\n✓ MAE masking visualization saved to figures/mae_masking_demo.png\")\n    \nexcept Exception as e:\n    print(f\"⚠️  Could not create masking visualization: {e}\")\n    import traceback\n    traceback.print_exc()\n\n# --- FIX 2: Removed all the duplicated code from here ---\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:51.608375Z","iopub.execute_input":"2025-11-05T04:10:51.608613Z","iopub.status.idle":"2025-11-05T04:10:52.509621Z","shell.execute_reply.started":"2025-11-05T04:10:51.608588Z","shell.execute_reply":"2025-11-05T04:10:52.508617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test MAE Feature Extraction\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING MAE FEATURE EXTRACTION\")\nprint(\"=\"*60)\n\n# Process sample image\ninputs = mae_processor(images=sample_img, return_tensors=\"pt\")\npixel_values = inputs['pixel_values'].to(DEVICE)\n\nprint(f\"Input tensor shape: {pixel_values.shape}\")\nprint(f\"  Batch size: {pixel_values.shape[0]}\")\nprint(f\"  Channels: {pixel_values.shape[1]} (RGB)\")\nprint(f\"  Height × Width: {pixel_values.shape[2]}×{pixel_values.shape[3]}\")\n\n# Extract features\nwith torch.no_grad():\n    outputs = mae_encoder(pixel_values)\n    features = outputs.last_hidden_state  # (batch, num_patches + 1, hidden_dim)\n\nprint(f\"\\nFeature tensor shape: {features.shape}\")\nprint(f\"  Batch size: {features.shape[0]}\")\nprint(f\"  Sequence length: {features.shape[1]} (196 patches + 1 [CLS] token)\")\nprint(f\"  Feature dimension: {features.shape[2]}\")\n\n# Extract CLS token (global representation)\ncls_token = features[:, 0, :]\nprint(f\"\\n[CLS] token (global feature): {cls_token.shape}\")\nprint(f\"  This will be used as the global image representation\")\nprint(f\"  Feature vector: {cls_token.shape[1]}-dimensional\")\n\n# Compute mean pooling (alternative aggregation)\nmean_pooled = features[:, 1:, :].mean(dim=1)\nprint(f\"\\nMean-pooled features: {mean_pooled.shape}\")\nprint(f\"  Averaged across all {num_patches} spatial patches\")\n\n# Feature statistics\nprint(f\"\\nFeature Statistics:\")\nprint(f\"  [CLS] token mean: {cls_token.mean().item():.4f}\")\nprint(f\"  [CLS] token std: {cls_token.std().item():.4f}\")\nprint(f\"  [CLS] token min: {cls_token.min().item():.4f}\")\nprint(f\"  [CLS] token max: {cls_token.max().item():.4f}\")\n\nprint(f\"\\n✓ MAE encoder successfully extracts {features.shape[2]}-dimensional features\")\nprint(f\"✓ Features are ready for downstream classification tasks\")\nprint(f\"✓ Feature extraction tested on device: {DEVICE}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:52.510512Z","iopub.execute_input":"2025-11-05T04:10:52.510762Z","iopub.status.idle":"2025-11-05T04:10:52.757564Z","shell.execute_reply.started":"2025-11-05T04:10:52.510717Z","shell.execute_reply":"2025-11-05T04:10:52.756678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Physics-Inspired Augmentation Functions\nprint(\"=\"*60)\nprint(\"IMPLEMENTING PHYSICS-INSPIRED AUGMENTATIONS\")\nprint(\"=\"*60)\n\ndef spectral_jitter(image, max_shift=0.05):\n    \"\"\"\n    Simulate spectral variations in imaging sensors.\n    Adds channel-wise Gaussian noise to simulate sensor variability.\n    \"\"\"\n    img = image.astype(np.float32) / 255.0\n    h, w, c = img.shape\n    \n    for ch in range(c):\n        shift = np.random.normal(0.0, max_shift)\n        img[..., ch] = np.clip(img[..., ch] + shift, 0.0, 1.0)\n    \n    return (img * 255).astype(np.uint8)\n\n\ndef add_dust_overlay(img, n_spots=150, max_radius=25):\n    \"\"\"\n    Simulate dust particles on leaf surfaces or camera lens.\n    Creates random circular spots with Gaussian blur.\n    \"\"\"\n    out = img.copy().astype(np.float32)\n    h, w = img.shape[:2]\n    mask = np.zeros((h, w), dtype=np.float32)\n    \n    for _ in range(n_spots):\n        x = np.random.randint(0, w)\n        y = np.random.randint(0, h)\n        r = np.random.randint(1, max_radius)\n        cv2.circle(mask, (x, y), r, np.random.uniform(0.02, 0.2), -1)\n    \n    mask = cv2.GaussianBlur(mask, (0, 0), sigmaX=5)\n    dust_color = np.random.uniform(100, 220)\n    \n    for c in range(out.shape[2]):\n        out[..., c] = out[..., c] * (1 - mask) + dust_color * mask\n    \n    return np.clip(out, 0, 255).astype(np.uint8)\n\n\ndef add_water_droplets(img, n_droplets=20):\n    \"\"\"\n    Simulate water droplets from dew or irrigation.\n    Creates lens-like distortions with highlights.\n    \"\"\"\n    out = img.copy()\n    h, w = img.shape[:2]\n    \n    for _ in range(n_droplets):\n        cx, cy = np.random.randint(20, w-20), np.random.randint(20, h-20)\n        radius = np.random.randint(5, 15)\n        \n        # Create droplet effect with brighter center\n        cv2.circle(out, (cx, cy), radius, (255, 255, 255), -1)\n        overlay = out.copy()\n        cv2.circle(overlay, (cx, cy), radius, \n                  tuple(np.clip(img[cy, cx] + 50, 0, 255).tolist()), -1)\n        alpha = 0.3\n        out = cv2.addWeighted(overlay, alpha, out, 1 - alpha, 0)\n    \n    return out\n\n\nprint(\"✓ Spectral jitter function implemented\")\nprint(\"✓ Dust overlay function implemented\")\nprint(\"✓ Water droplet function implemented\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:52.758293Z","iopub.execute_input":"2025-11-05T04:10:52.758549Z","iopub.status.idle":"2025-11-05T04:10:52.770013Z","shell.execute_reply.started":"2025-11-05T04:10:52.758532Z","shell.execute_reply":"2025-11-05T04:10:52.769121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Albumentations Pipeline with Physics-Inspired Augmentations\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING AUGMENTATION PIPELINE\")\nprint(\"=\"*60)\n\ndef get_train_transforms(img_size=224):\n    \"\"\"Training augmentation pipeline with physics-inspired effects.\"\"\"\n    return A.Compose([\n        A.Resize(img_size, img_size),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.2),\n        A.RandomRotate90(p=0.3),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n        \n        # Physics-inspired: Environmental conditions\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n        A.RandomGamma(gamma_limit=(80, 120), p=0.4),\n        \n        # Physics-inspired: Atmospheric effects\n        A.RandomFog(fog_coef_lower=0.05, fog_coef_upper=0.25, alpha_coef=0.1, p=0.3),\n        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, p=0.3),\n        A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0, angle_upper=1, \n                         num_flare_circles_lower=1, num_flare_circles_upper=2, p=0.1),\n        \n        # Physics-inspired: Image quality degradation\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n        A.MotionBlur(blur_limit=7, p=0.3),\n        A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n        \n        # Physics-inspired: Occlusions\n        A.CoarseDropout(max_holes=8, max_height=30, max_width=30, \n                        min_holes=1, min_height=10, min_width=10,\n                        fill_value=0, p=0.4),\n        \n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n\ndef get_val_transforms(img_size=224):\n    \"\"\"Validation/test transforms (no augmentation).\"\"\"\n    return A.Compose([\n        A.Resize(img_size, img_size),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n\ntrain_transform = get_train_transforms(img_size=224)\nval_transform = get_val_transforms(img_size=224)\n\nprint(\"✓ Training augmentation pipeline created:\")\nprint(\"  • Geometric: Flip, Rotate, Shift, Scale\")\nprint(\"  • Color: Brightness, Contrast, Hue, Saturation, Gamma\")\nprint(\"  • Atmospheric: Fog, Shadow, Sun Flare\")\nprint(\"  • Degradation: Gaussian Noise, Blur, Motion Blur, ISO Noise\")\nprint(\"  • Occlusion: Coarse Dropout\")\nprint(\"  • Normalization: ImageNet statistics\")\nprint(\"\\n✓ Validation transform created (resize + normalize only)\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:52.771166Z","iopub.execute_input":"2025-11-05T04:10:52.771439Z","iopub.status.idle":"2025-11-05T04:10:52.803895Z","shell.execute_reply.started":"2025-11-05T04:10:52.771415Z","shell.execute_reply":"2025-11-05T04:10:52.803078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Augmentations on Sample Images\nprint(\"\\n\" + \"=\"*60)\nprint(\"VISUALIZING AUGMENTATION EFFECTS\")\nprint(\"=\"*60)\n\n# Select 3 sample images from different classes\nsample_indices = [0, len(train_paths)//3, 2*len(train_paths)//3]\nsample_images = [Image.open(train_paths[i]).convert('RGB') for i in sample_indices]\n\n# Create augmentation visualization\nfig, axes = plt.subplots(4, 4, figsize=(16, 16))\nfig.suptitle('Physics-Inspired Augmentation Examples', fontsize=16, fontweight='bold')\n\naugmentation_types = [\n    ('Original', None),\n    ('Spectral Jitter', lambda x: spectral_jitter(np.array(x))),\n    ('Dust Overlay', lambda x: add_dust_overlay(np.array(x), n_spots=100)),\n    ('Brightness+Contrast', A.Compose([A.RandomBrightnessContrast(p=1.0)])),\n    ('Fog Effect', A.Compose([A.RandomFog(fog_coef_lower=0.2, fog_coef_upper=0.4, p=1.0)])),\n    ('Shadow', A.Compose([A.RandomShadow(num_shadows_lower=2, num_shadows_upper=3, p=1.0)])),\n    ('Gaussian Noise', A.Compose([A.GaussNoise(var_limit=(30.0, 60.0), p=1.0)])),\n    ('Motion Blur', A.Compose([A.MotionBlur(blur_limit=9, p=1.0)])),\n    ('Geometric', A.Compose([A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=1.0)])),\n    ('Coarse Dropout', A.Compose([A.CoarseDropout(max_holes=10, max_height=40, max_width=40, p=1.0)])),\n    ('Water Droplets', lambda x: add_water_droplets(np.array(x), n_droplets=15)),\n    ('Combined', train_transform)\n]\n\n# Show first sample with various augmentations\nsample_img = sample_images[0].resize((224, 224))\nsample_array = np.array(sample_img)\n\nfor idx, (aug_name, aug_fn) in enumerate(augmentation_types[:12]):\n    row = idx // 4\n    col = idx % 4\n    \n    if aug_name == 'Original':\n        axes[row, col].imshow(sample_array)\n    elif aug_name == 'Combined':\n        # For tensor output, need to denormalize\n        augmented = aug_fn(image=sample_array)['image']\n        if torch.is_tensor(augmented):\n            augmented = augmented.permute(1, 2, 0).numpy()\n            # Denormalize\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            augmented = std * augmented + mean\n            augmented = np.clip(augmented * 255, 0, 255).astype(np.uint8)\n        axes[row, col].imshow(augmented)\n    elif callable(aug_fn) and not isinstance(aug_fn, A.Compose):\n        # Custom function\n        augmented = aug_fn(sample_img)\n        axes[row, col].imshow(augmented)\n    else:\n        # Albumentations\n        augmented = aug_fn(image=sample_array)['image']\n        axes[row, col].imshow(augmented)\n    \n    axes[row, col].set_title(aug_name, fontsize=10, fontweight='bold')\n    axes[row, col].axis('off')\n\n# Hide remaining subplots\nfor idx in range(len(augmentation_types), 16):\n    row = idx // 4\n    col = idx % 4\n    axes[row, col].axis('off')\n\nplt.tight_layout()\nplt.savefig('figures/augmentation_examples.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Augmentation visualization saved to figures/augmentation_examples.png\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:10:52.804766Z","iopub.execute_input":"2025-11-05T04:10:52.805016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create PyTorch Dataset with Augmentations\nclass PlantDiseaseDataset(Dataset):\n    \"\"\"PyTorch dataset for plant disease images with augmentation support.\"\"\"\n    \n    def __init__(self, image_paths, labels, transform=None, apply_custom_aug=True):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.apply_custom_aug = apply_custom_aug\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        image = np.array(image)\n        \n        # Apply custom physics-inspired augmentations (randomly)\n        if self.apply_custom_aug and self.transform is not None:\n            if np.random.rand() < 0.3:\n                image = spectral_jitter(image, max_shift=0.05)\n            if np.random.rand() < 0.2:\n                image = add_dust_overlay(image, n_spots=np.random.randint(50, 150))\n            if np.random.rand() < 0.15:\n                image = add_water_droplets(image, n_droplets=np.random.randint(10, 25))\n        \n        # Apply albumentations transform\n        if self.transform is not None:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        \n        label = self.labels[idx]\n        \n        return image, label\n\n\n# Create datasets\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING PYTORCH DATASETS\")\nprint(\"=\"*60)\n\ntrain_dataset = PlantDiseaseDataset(\n    train_paths, train_labels, \n    transform=train_transform, \n    apply_custom_aug=True\n)\n\nval_dataset = PlantDiseaseDataset(\n    val_paths, val_labels, \n    transform=val_transform, \n    apply_custom_aug=False\n)\n\ntest_dataset = PlantDiseaseDataset(\n    test_paths, test_labels, \n    transform=val_transform, \n    apply_custom_aug=False\n)\n\nprint(f\"✓ Training dataset: {len(train_dataset)} samples (with augmentation)\")\nprint(f\"✓ Validation dataset: {len(val_dataset)} samples (no augmentation)\")\nprint(f\"✓ Test dataset: {len(test_dataset)} samples (no augmentation)\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Pre-trained SegFormer Model\nprint(\"=\"*60)\nprint(\"LOADING SEGFORMER SEGMENTATION MODEL\")\nprint(\"=\"*60)\n\ntry:\n    from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n    \n    # Load SegFormer-B0 (smallest, fastest variant)\n    segformer_model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n    \n    print(f\"Loading SegFormer-B0 model: {segformer_model_name}\")\n    \n    segformer_processor = SegformerImageProcessor.from_pretrained(segformer_model_name)\n    segformer_model = SegformerForSemanticSegmentation.from_pretrained(segformer_model_name)\n    \n    # Move to GPU\n    segformer_model = segformer_model.to(DEVICE)\n    segformer_model.eval()\n    \n    print(f\"✓ SegFormer-B0 loaded successfully\")\n    print(f\"  Architecture: Mix Transformer Encoder + Lightweight Decoder\")\n    print(f\"  Pretrained on: ADE20K (150 classes)\")\n    print(f\"  Input size: 512×512 pixels\")\n    print(f\"  Parameters: ~3.8M\")\n    print(f\"  Inference speed: ~50 FPS (GPU)\")\n    \n    # Note: For plant disease segmentation, this model would be fine-tuned\n    # on a dataset with lesion annotations. For this demo, we use it as-is\n    # to demonstrate the segmentation capability.\n    \n    print(f\"\\n⚠️  Note: This is a pretrained model for demonstration.\")\n    print(f\"   In production, fine-tune on plant lesion segmentation dataset.\")\n    \nexcept Exception as e:\n    print(f\"⚠️  Warning: Could not load SegFormer model: {e}\")\n    print(\"   Segmentation stage will be skipped or use alternative method.\")\n    segformer_model = None\n    segformer_processor = None\n\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Demonstrate Segmentation on Sample Images\nif segformer_model is not None:\n    print(\"\\n\" + \"=\"*60)\n    print(\"RUNNING SEGMENTATION INFERENCE\")\n    print(\"=\"*60)\n    \n    # Select diverse sample images\n    sample_indices = [0, len(train_paths)//4, len(train_paths)//2, 3*len(train_paths)//4]\n    \n    fig, axes = plt.subplots(len(sample_indices), 3, figsize=(15, 5*len(sample_indices)))\n    fig.suptitle('SegFormer Segmentation Examples (Pretrained ADE20K)', fontsize=16, fontweight='bold')\n    \n    for row_idx, img_idx in enumerate(sample_indices):\n        # Load and process image\n        img_path = train_paths[img_idx]\n        image = Image.open(img_path).convert('RGB')\n        image_resized = image.resize((512, 512))\n        \n        # Run segmentation\n        inputs = segformer_processor(images=image_resized, return_tensors=\"pt\")\n        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = segformer_model(**inputs)\n            logits = outputs.logits  # (1, num_classes, H, W)\n        \n        # Get predicted segmentation\n        upsampled_logits = torch.nn.functional.interpolate(\n            logits,\n            size=image_resized.size[::-1],\n            mode=\"bilinear\",\n            align_corners=False\n        )\n        pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n        \n        # Create binary mask (any segmented region vs background)\n        # For demo purposes, we consider any non-background class as potential lesion\n        binary_mask = (pred_seg > 0).astype(np.uint8) * 255\n        \n        # Visualize\n        if len(sample_indices) == 1:\n            ax_img, ax_mask, ax_overlay = axes\n        else:\n            ax_img, ax_mask, ax_overlay = axes[row_idx]\n        \n        # Original image\n        ax_img.imshow(image_resized)\n        ax_img.set_title(f'Original Image\\n{Path(img_path).parent.name}', fontsize=10)\n        ax_img.axis('off')\n        \n        # Segmentation mask\n        ax_mask.imshow(binary_mask, cmap='hot')\n        ax_mask.set_title('Segmentation Mask', fontsize=10)\n        ax_mask.axis('off')\n        \n        # Overlay\n        overlay = np.array(image_resized).copy()\n        overlay[binary_mask > 0] = overlay[binary_mask > 0] * 0.5 + np.array([255, 0, 0]) * 0.5\n        ax_overlay.imshow(overlay.astype(np.uint8))\n        ax_overlay.set_title('Overlay (Red = Segmented)', fontsize=10)\n        ax_overlay.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('figures/segmentation_examples.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"✓ Segmentation examples saved to figures/segmentation_examples.png\")\n    print(f\"✓ SegFormer successfully segments regions of interest\")\n    print(f\"\\n⚠️  Note: These masks are from pretrained ADE20K model.\")\n    print(f\"   For accurate lesion segmentation, fine-tune on annotated plant disease data.\")\n    print(\"=\"*60)\nelse:\n    print(\"⚠️  Segmentation model not available. Skipping visualization.\")","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-11-05T04:11:08.582184Z","shell.execute_reply.started":"2025-11-05T04:10:57.55786Z","shell.execute_reply":"2025-11-05T04:11:08.58087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Morphometric Feature Extraction Functions\nprint(\"=\"*60)\nprint(\"IMPLEMENTING MORPHOMETRIC ANALYSIS\")\nprint(\"=\"*60)\n\ndef compute_morphometrics(mask):\n    \"\"\"\n    Compute morphological features from binary segmentation mask.\n    \n    Args:\n        mask: Binary mask (H, W) with values 0 (background) or 255 (lesion)\n    \n    Returns:\n        dict: Morphometric features (area, perimeter, eccentricity, solidity)\n    \"\"\"\n    # Convert to binary\n    binary_mask = (mask > 127).astype(np.uint8)\n    \n    # Extract connected components\n    props = measure.regionprops(measure.label(binary_mask))\n    \n    if len(props) == 0:\n        # No lesions detected\n        return {\n            'area': 0.0,\n            'perimeter': 0.0,\n            'eccentricity': 0.0,\n            'solidity': 0.0\n        }\n    \n    # Aggregate features from all detected lesions\n    areas = []\n    perimeters = []\n    eccentricities = []\n    solidities = []\n    \n    for prop in props:\n        areas.append(prop.area)\n        perimeters.append(prop.perimeter)\n        \n        # Eccentricity (shape elongation: 0=circle, 1=line)\n        try:\n            eccentricities.append(prop.eccentricity)\n        except:\n            eccentricities.append(0.0)\n        \n        # Solidity (convexity: area/convex_hull_area)\n        try:\n            solidities.append(prop.solidity)\n        except:\n            solidities.append(1.0)\n    \n    return {\n        'area': float(np.sum(areas)),  # Total lesion area (pixels)\n        'perimeter': float(np.sum(perimeters)),  # Total perimeter length\n        'eccentricity': float(np.mean(eccentricities)),  # Average shape elongation\n        'solidity': float(np.mean(solidities))  # Average convexity\n    }\n\n\ndef extract_morphometric_features(image, mask):\n    \"\"\"\n    Extract and normalize morphometric features.\n    \n    Args:\n        image: RGB image (H, W, 3)\n        mask: Binary mask (H, W)\n    \n    Returns:\n        np.array: Normalized 4D feature vector\n    \"\"\"\n    morpho = compute_morphometrics(mask)\n    \n    # Normalize features (empirical ranges for 224×224 images)\n    area_norm = morpho['area'] / (224 * 224)  # Normalize by image size\n    perimeter_norm = morpho['perimeter'] / (4 * 224)  # Normalize by image perimeter\n    eccentricity_norm = morpho['eccentricity']  # Already 0-1\n    solidity_norm = morpho['solidity']  # Already 0-1\n    \n    return np.array([area_norm, perimeter_norm, eccentricity_norm, solidity_norm], \n                    dtype=np.float32)\n\n\nprint(\"✓ Morphometric extraction functions implemented:\")\nprint(\"  • Area (total lesion area in pixels)\")\nprint(\"  • Perimeter (total lesion boundary length)\")\nprint(\"  • Eccentricity (shape elongation: 0=circle, 1=line)\")\nprint(\"  • Solidity (convexity: area/convex_hull_area)\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:08.583272Z","iopub.execute_input":"2025-11-05T04:11:08.583641Z","iopub.status.idle":"2025-11-05T04:11:08.595556Z","shell.execute_reply.started":"2025-11-05T04:11:08.583612Z","shell.execute_reply":"2025-11-05T04:11:08.594782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Demonstrate Morphometric Feature Extraction\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXTRACTING MORPHOMETRIC FEATURES FROM SAMPLE MASKS\")\nprint(\"=\"*60)\n\n# Create synthetic masks for demonstration (since we don't have ground truth masks)\n# In production, these would come from the segmentation model or manual annotations\n\nsample_morphometrics = []\n\nfor i in range(5):\n    # Generate synthetic mask with disease-like patterns\n    mask = np.zeros((224, 224), dtype=np.uint8)\n    \n    # Add 1-3 lesion regions\n    num_lesions = np.random.randint(1, 4)\n    for _ in range(num_lesions):\n        cx = np.random.randint(50, 174)\n        cy = np.random.randint(50, 174)\n        \n        if np.random.rand() < 0.5:\n            # Circular lesion\n            radius = np.random.randint(10, 40)\n            cv2.circle(mask, (cx, cy), radius, 255, -1)\n        else:\n            # Elliptical lesion (more elongated)\n            axes = (np.random.randint(15, 50), np.random.randint(10, 30))\n            angle = np.random.randint(0, 180)\n            cv2.ellipse(mask, (cx, cy), axes, angle, 0, 360, 255, -1)\n    \n    # Extract morphometrics\n    morpho = compute_morphometrics(mask)\n    morpho_features = extract_morphometric_features(None, mask)\n    \n    sample_morphometrics.append({\n        'mask_id': i,\n        'raw_features': morpho,\n        'normalized_features': morpho_features\n    })\n    \n    print(f\"\\nSample {i+1}:\")\n    print(f\"  Area: {morpho['area']:.1f} px ({morpho_features[0]:.3f} normalized)\")\n    print(f\"  Perimeter: {morpho['perimeter']:.1f} px ({morpho_features[1]:.3f} normalized)\")\n    print(f\"  Eccentricity: {morpho['eccentricity']:.3f} (0=circle, 1=line)\")\n    print(f\"  Solidity: {morpho['solidity']:.3f} (1=convex)\")\n\n# Visualize sample masks with morphometrics\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfig.suptitle('Morphometric Feature Extraction Examples', fontsize=14, fontweight='bold')\n\nfor i, (ax, sample) in enumerate(zip(axes, sample_morphometrics)):\n    # Recreate mask for visualization\n    mask = np.zeros((224, 224), dtype=np.uint8)\n    num_lesions = np.random.randint(1, 4)\n    for _ in range(num_lesions):\n        cx = np.random.randint(50, 174)\n        cy = np.random.randint(50, 174)\n        radius = np.random.randint(10, 40)\n        cv2.circle(mask, (cx, cy), radius, 255, -1)\n    \n    ax.imshow(mask, cmap='hot')\n    ax.set_title(f\"Sample {i+1}\\nArea: {sample['raw_features']['area']:.0f}px\\n\"\n                 f\"Ecc: {sample['raw_features']['eccentricity']:.2f}\", fontsize=9)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.savefig('figures/morphometric_examples.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n✓ Morphometric extraction demonstrated on 5 synthetic masks\")\nprint(f\"✓ Visualization saved to figures/morphometric_examples.png\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:08.596502Z","iopub.execute_input":"2025-11-05T04:11:08.596738Z","iopub.status.idle":"2025-11-05T04:11:09.934914Z","shell.execute_reply.started":"2025-11-05T04:11:08.596723Z","shell.execute_reply":"2025-11-05T04:11:09.934054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Morphometric Feature Projection MLP\nclass MorphoMLP(nn.Module):\n    \"\"\"\n    Multi-layer perceptron for projecting morphometric features.\n    Maps 4D morphometric vector to higher-dimensional embedding space.\n    \"\"\"\n    \n    def __init__(self, input_dim=4, hidden_dim=64, output_dim=512):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, output_dim),\n            nn.ReLU()\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (batch, 4) morphometric features\n        Returns:\n            (batch, output_dim) projected features\n        \"\"\"\n        return self.mlp(x)\n\n\n# Test MorphoMLP\nprint(\"\\n\" + \"=\"*60)\nprint(\"DEFINING MORPHOMETRIC FEATURE PROJECTION MLP\")\nprint(\"=\"*60)\n\nmorpho_mlp = MorphoMLP(input_dim=4, hidden_dim=64, output_dim=512).to(DEVICE)\n\n# Test with sample features\nsample_morpho_batch = torch.tensor([\n    [0.15, 0.25, 0.6, 0.85],  # Medium area, elongated, convex\n    [0.05, 0.10, 0.2, 0.95],  # Small area, circular, very convex\n    [0.30, 0.50, 0.8, 0.70],  # Large area, very elongated, less convex\n], dtype=torch.float32).to(DEVICE)\n\nwith torch.no_grad():\n    morpho_embeddings = morpho_mlp(sample_morpho_batch)\n\nprint(f\"✓ MorphoMLP defined: 4 → 64 → 512\")\nprint(f\"  Input shape: {sample_morpho_batch.shape}\")\nprint(f\"  Output shape: {morpho_embeddings.shape}\")\nprint(f\"  Parameters: {sum(p.numel() for p in morpho_mlp.parameters()):,}\")\nprint(f\"\\n✓ MorphoMLP successfully projects morphometric features to 512-dim space\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:09.935776Z","iopub.execute_input":"2025-11-05T04:11:09.936773Z","iopub.status.idle":"2025-11-05T04:11:09.961696Z","shell.execute_reply.started":"2025-11-05T04:11:09.936753Z","shell.execute_reply":"2025-11-05T04:11:09.960903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Hybrid Multi-Branch Classifier\nprint(\"=\"*60)\nprint(\"DEFINING HYBRID MULTI-BRANCH FUSION CLASSIFIER\")\nprint(\"=\"*60)\n\nclass HybridClassifier(nn.Module):\n    \"\"\"\n    Hybrid Multi-Branch Fusion Classifier for Plant Disease Diagnosis.\n    \n    Combines three complementary feature streams:\n    1. ViT (Vision Transformer) - Global context and attention\n    2. EfficientNet-B0 (CNN) - Local texture patterns\n    3. Morphometric MLP - Explicit shape features\n    \"\"\"\n    \n    def __init__(self, \n                 num_classes=38,\n                 vit_dim=768,\n                 cnn_dim=1280,\n                 morpho_dim=512,\n                 fusion_dim=512,\n                 dropout=0.3):\n        super().__init__()\n        \n        # Branch 1: ViT Encoder (from MAE)\n        self.vit = mae_encoder\n        self.vit_proj = nn.Sequential(\n            nn.Linear(vit_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # Branch 2: EfficientNet-B0 CNN\n        efficientnet = torchvision.models.efficientnet_b0(pretrained=True)\n        self.cnn = nn.Sequential(*list(efficientnet.children())[:-1])  # Remove classifier\n        self.cnn_proj = nn.Sequential(\n            nn.Linear(cnn_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # Branch 3: Morphometric MLP\n        self.morpho_mlp = MorphoMLP(input_dim=4, hidden_dim=64, output_dim=morpho_dim)\n        \n        # Fusion and Classification Head\n        total_dim = fusion_dim * 2 + morpho_dim  # ViT + CNN + Morpho\n        self.fusion = nn.Sequential(\n            nn.Linear(total_dim, fusion_dim * 2),\n            nn.ReLU(),\n            nn.BatchNorm1d(fusion_dim * 2),\n            nn.Dropout(dropout),\n            nn.Linear(fusion_dim * 2, fusion_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(fusion_dim),\n            nn.Dropout(dropout),\n            nn.Linear(fusion_dim, num_classes)\n        )\n        \n        # Initialize weights\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize projection and fusion layers.\"\"\"\n        for m in [self.vit_proj, self.cnn_proj, self.fusion]:\n            for layer in m.modules():\n                if isinstance(layer, nn.Linear):\n                    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n                    if layer.bias is not None:\n                        nn.init.constant_(layer.bias, 0)\n                elif isinstance(layer, nn.BatchNorm1d):\n                    nn.init.constant_(layer.weight, 1)\n                    nn.init.constant_(layer.bias, 0)\n    \n    def forward(self, images, morpho_features=None):\n        \"\"\"\n        Forward pass through hybrid classifier.\n        \n        Args:\n            images: (batch, 3, 224, 224) normalized RGB images\n            morpho_features: (batch, 4) morphometric features (optional)\n        \n        Returns:\n            logits: (batch, num_classes) class logits\n            embeddings: dict with intermediate features\n        \"\"\"\n        batch_size = images.size(0)\n        \n        # Branch 1: ViT features\n        vit_out = self.vit(pixel_values=images).last_hidden_state  # (batch, 197, 768)\n        vit_feat = vit_out[:, 0, :]  # [CLS] token (batch, 768)\n        vit_emb = self.vit_proj(vit_feat)  # (batch, 512)\n        \n        # Branch 2: CNN features\n        cnn_feat = self.cnn(images)  # (batch, 1280, 1, 1)\n        cnn_feat = cnn_feat.flatten(1)  # (batch, 1280)\n        cnn_emb = self.cnn_proj(cnn_feat)  # (batch, 512)\n        \n        # Branch 3: Morphometric features\n        if morpho_features is None:\n            # Use zeros if morphometric features not provided\n            morpho_features = torch.zeros(batch_size, 4, device=images.device)\n        morpho_emb = self.morpho_mlp(morpho_features)  # (batch, 512)\n        \n        # Concatenate all features\n        fused = torch.cat([vit_emb, cnn_emb, morpho_emb], dim=1)  # (batch, 1536)\n        \n        # Classification head\n        logits = self.fusion(fused)  # (batch, num_classes)\n        \n        embeddings = {\n            'vit': vit_emb,\n            'cnn': cnn_emb,\n            'morpho': morpho_emb,\n            'fused': fused\n        }\n        \n        return logits, embeddings\n\n\nprint(\"✓ HybridClassifier architecture defined\")\nprint(\"\\nArchitecture Summary:\")\nprint(\"  Branch 1: ViT-Base/16 (MAE encoder) → 768-dim → 512-dim\")\nprint(\"  Branch 2: EfficientNet-B0 → 1280-dim → 512-dim\")\nprint(\"  Branch 3: MorphoMLP → 4-dim → 512-dim\")\nprint(\"  Fusion: Concat(512 + 512 + 512) → 1024 → 512 → num_classes\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:09.96253Z","iopub.execute_input":"2025-11-05T04:11:09.96277Z","iopub.status.idle":"2025-11-05T04:11:09.975106Z","shell.execute_reply.started":"2025-11-05T04:11:09.962753Z","shell.execute_reply":"2025-11-05T04:11:09.974352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate and Analyze Model\nprint(\"\\n\" + \"=\"*60)\nprint(\"INSTANTIATING HYBRID CLASSIFIER\")\nprint(\"=\"*60)\n\nmodel = HybridClassifier(\n    num_classes=NUM_CLASSES,\n    vit_dim=768,\n    cnn_dim=1280,\n    morpho_dim=512,\n    fusion_dim=512,\n    dropout=0.3\n).to(DEVICE)\n\n# Count parameters\ndef count_parameters(model):\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable\n\ntotal_params, trainable_params = count_parameters(model)\n\nprint(f\"✓ Model instantiated on {DEVICE}\")\nprint(f\"\\nModel Statistics:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Number of classes: {NUM_CLASSES}\")\n\n# Analyze parameter distribution\nvit_params = sum(p.numel() for p in model.vit.parameters())\ncnn_params = sum(p.numel() for p in model.cnn.parameters())\nmorpho_params = sum(p.numel() for p in model.morpho_mlp.parameters())\nfusion_params = sum(p.numel() for p in model.fusion.parameters())\nproj_params = sum(p.numel() for p in model.vit_proj.parameters()) + \\\n              sum(p.numel() for p in model.cnn_proj.parameters())\n\nprint(f\"\\nParameter Distribution:\")\nprint(f\"  ViT Encoder: {vit_params:,} ({vit_params/total_params*100:.1f}%)\")\nprint(f\"  CNN Encoder: {cnn_params:,} ({cnn_params/total_params*100:.1f}%)\")\nprint(f\"  Morpho MLP: {morpho_params:,} ({morpho_params/total_params*100:.1f}%)\")\nprint(f\"  Projections: {proj_params:,} ({proj_params/total_params*100:.1f}%)\")\nprint(f\"  Fusion Head: {fusion_params:,} ({fusion_params/total_params*100:.1f}%)\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:09.976024Z","iopub.execute_input":"2025-11-05T04:11:09.976239Z","iopub.status.idle":"2025-11-05T04:11:10.620419Z","shell.execute_reply.started":"2025-11-05T04:11:09.976224Z","shell.execute_reply":"2025-11-05T04:11:10.61956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test Forward Pass with Sample Batch\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING FORWARD PASS\")\nprint(\"=\"*60)\n\nmodel.eval()\n\n# Create sample batch\nsample_images = []\nsample_morpho = []\n\nfor i in range(4):\n    img = Image.open(train_paths[i]).convert('RGB')\n    img_array = np.array(img)\n    \n    # Apply transform\n    augmented = val_transform(image=img_array)\n    img_tensor = augmented['image']\n    sample_images.append(img_tensor)\n    \n    # Create dummy morphometric features\n    morpho = np.random.rand(4).astype(np.float32)\n    sample_morpho.append(morpho)\n\nsample_batch = torch.stack(sample_images).to(DEVICE)\nmorpho_batch = torch.tensor(np.stack(sample_morpho)).to(DEVICE)\n\nprint(f\"Sample batch shape: {sample_batch.shape}\")\nprint(f\"Morphometric batch shape: {morpho_batch.shape}\")\n\n# Forward pass\nwith torch.no_grad():\n    logits, embeddings = model(sample_batch, morpho_batch)\n\nprint(f\"\\n✓ Forward pass successful!\")\nprint(f\"  Logits shape: {logits.shape}\")\nprint(f\"  ViT embedding: {embeddings['vit'].shape}\")\nprint(f\"  CNN embedding: {embeddings['cnn'].shape}\")\nprint(f\"  Morpho embedding: {embeddings['morpho'].shape}\")\nprint(f\"  Fused embedding: {embeddings['fused'].shape}\")\n\n# Get predictions\nprobs = torch.softmax(logits, dim=1)\npreds = logits.argmax(dim=1)\n\nprint(f\"\\nSample Predictions:\")\nfor i in range(4):\n    pred_class = PLANTVILLAGE_CLASSES[preds[i]]\n    confidence = probs[i, preds[i]].item()\n    print(f\"  Image {i+1}: {pred_class} (confidence: {confidence:.2%})\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:10.621268Z","iopub.execute_input":"2025-11-05T04:11:10.621836Z","iopub.status.idle":"2025-11-05T04:11:10.705464Z","shell.execute_reply.started":"2025-11-05T04:11:10.621816Z","shell.execute_reply":"2025-11-05T04:11:10.704514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Model Architecture\nprint(\"\\n\" + \"=\"*60)\nprint(\"VISUALIZING MODEL ARCHITECTURE\")\nprint(\"=\"*60)\n\nfig, ax = plt.subplots(figsize=(14, 10))\nax.axis('off')\n\n# Architecture diagram (text-based)\narchitecture_text = \"\"\"\n┌─────────────────────────────────────────────────────────────────┐\n│                    HYBRID MULTI-BRANCH CLASSIFIER                │\n└─────────────────────────────────────────────────────────────────┘\n\n                         Input Image (224×224×3)\n                                    │\n            ┌───────────────────────┼───────────────────────┐\n            │                       │                       │\n            ▼                       ▼                       ▼\n    ┌───────────────┐      ┌───────────────┐      ┌───────────────┐\n    │  Branch 1:    │      │  Branch 2:    │      │  Branch 3:    │\n    │  ViT-Base/16  │      │ EfficientNet  │      │   Morpho MLP  │\n    │  (MAE Encoder)│      │      -B0      │      │               │\n    └───────┬───────┘      └───────┬───────┘      └───────┬───────┘\n            │                      │                      │\n            │                      │                      │\n     [CLS] Token              AdaptiveAvg           Morphometric\n       (768-dim)                (1280-dim)           Features (4-dim)\n            │                      │                      │\n            ▼                      ▼                      ▼\n    ┌───────────────┐      ┌───────────────┐      ┌───────────────┐\n    │  Linear(512)  │      │  Linear(512)  │      │  4→64→512     │\n    │  + ReLU       │      │  + ReLU       │      │  + ReLU       │\n    │  + Dropout    │      │  + Dropout    │      │  + BN         │\n    └───────┬───────┘      └───────┬───────┘      └───────┬───────┘\n            │                      │                      │\n            └──────────────────────┼──────────────────────┘\n                                   │\n                            Concatenate\n                             (1536-dim)\n                                   │\n                                   ▼\n                        ┌──────────────────┐\n                        │  Fusion Head     │\n                        │  1536 → 1024     │\n                        │  + ReLU + BN     │\n                        │  1024 → 512      │\n                        │  + ReLU + BN     │\n                        │  512 → 38 classes│\n                        └────────┬─────────┘\n                                 │\n                                 ▼\n                          Class Logits (38)\n                                 │\n                                 ▼\n                            Softmax\n                                 │\n                                 ▼\n                        Disease Prediction\n\"\"\"\n\nax.text(0.5, 0.5, architecture_text, \n        fontsize=9, \n        family='monospace',\n        va='center', \n        ha='center',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n\nax.set_title('Hybrid Multi-Branch Fusion Classifier Architecture', \n             fontsize=14, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.savefig('figures/hybrid_architecture.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Architecture diagram saved to figures/hybrid_architecture.png\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:10.706372Z","iopub.execute_input":"2025-11-05T04:11:10.70679Z","iopub.status.idle":"2025-11-05T04:11:11.804367Z","shell.execute_reply.started":"2025-11-05T04:11:10.706766Z","shell.execute_reply":"2025-11-05T04:11:11.803638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========== TRAINING CONFIGURATION ==========\n\n# Training mode\nRUN_SMOKE_TEST = USE_SMOKE_TEST  # Quick 2-epoch test\nRUN_FULL_TRAINING = not USE_SMOKE_TEST  # Full training\n\n# Hyperparameters\nif RUN_SMOKE_TEST:\n    NUM_EPOCHS = 2\n    BATCH_SIZE = 8\n    NUM_WORKERS = 2\nelse:\n    NUM_EPOCHS = 15\n    BATCH_SIZE = 96\n    NUM_WORKERS = 4\n\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 0.01\nWARMUP_EPOCHS = 5\nPATIENCE = 10  # Early stopping patience\nSAVE_EVERY = 5  # Save checkpoint every N epochs\n\nprint(\"=\"*60)\nprint(\"TRAINING CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"Mode: {'SMOKE TEST' if RUN_SMOKE_TEST else 'FULL TRAINING'}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Weight decay: {WEIGHT_DECAY}\")\nprint(f\"Warmup epochs: {WARMUP_EPOCHS}\")\nprint(f\"Early stopping patience: {PATIENCE}\")\nprint(f\"Number of workers: {NUM_WORKERS}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:11.805249Z","iopub.execute_input":"2025-11-05T04:11:11.805558Z","iopub.status.idle":"2025-11-05T04:11:11.812364Z","shell.execute_reply.started":"2025-11-05T04:11:11.805534Z","shell.execute_reply":"2025-11-05T04:11:11.811553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Data Loaders\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING DATA LOADERS\")\nprint(\"=\"*60)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    drop_last=True  # For stable batch normalization\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nprint(f\"✓ Train loader: {len(train_loader)} batches ({len(train_dataset)} samples)\")\nprint(f\"✓ Val loader: {len(val_loader)} batches ({len(val_dataset)} samples)\")\nprint(f\"✓ Test loader: {len(test_loader)} batches ({len(test_dataset)} samples)\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:11.813154Z","iopub.execute_input":"2025-11-05T04:11:11.813404Z","iopub.status.idle":"2025-11-05T04:11:11.831781Z","shell.execute_reply.started":"2025-11-05T04:11:11.813381Z","shell.execute_reply":"2025-11-05T04:11:11.831002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup Training Components\nprint(\"\\n\" + \"=\"*60)\nprint(\"SETTING UP TRAINING COMPONENTS\")\nprint(\"=\"*60)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer (AdamW with weight decay)\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    betas=(0.9, 0.999)\n)\n\n# Learning rate scheduler (Cosine Annealing with Warmup)\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n\nwarmup_scheduler = LinearLR(\n    optimizer, \n    start_factor=0.1, \n    end_factor=1.0, \n    total_iters=WARMUP_EPOCHS\n)\n\ncosine_scheduler = CosineAnnealingLR(\n    optimizer,\n    T_max=NUM_EPOCHS - WARMUP_EPOCHS,\n    eta_min=1e-6\n)\n\nscheduler = SequentialLR(\n    optimizer,\n    schedulers=[warmup_scheduler, cosine_scheduler],\n    milestones=[WARMUP_EPOCHS]\n)\n\nprint(f\"✓ Loss function: Cross-Entropy\")\nprint(f\"✓ Optimizer: AdamW (lr={LEARNING_RATE}, wd={WEIGHT_DECAY})\")\nprint(f\"✓ Scheduler: Linear Warmup ({WARMUP_EPOCHS} epochs) + Cosine Annealing\")\nprint(f\"✓ Gradient clipping: Max norm 1.0\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:11.835486Z","iopub.execute_input":"2025-11-05T04:11:11.835683Z","iopub.status.idle":"2025-11-05T04:11:11.853118Z","shell.execute_reply.started":"2025-11-05T04:11:11.835668Z","shell.execute_reply":"2025-11-05T04:11:11.852359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function for \"real\" (but fast) morphometrics\n# This uses non-DL segmentation (HSV Color Masking) for speed.\n\n# We must de-normalize the images. \n# Assuming standard ImageNet normalization was used in the dataset pipeline.\nIMG_MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\nIMG_STD = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n\ndef get_real_morpho_features_hsv(image_batch_tensor):\n    \"\"\"\n    Calculates morphometric features ON-THE-FLY from a batch of images\n    using fast HSV color-masking (segmenting green).\n    \n    This is a fast, non-DL proxy for the segmentation pipeline.\n    \"\"\"\n    \n    # 1. Move batch to CPU, de-normalize, and convert to NumPy\n    # De-normalize\n    images_denorm = (image_batch_tensor * IMG_STD) + IMG_MEAN\n    # Clamp to [0, 1] and convert to [0, 255] uint8\n    images_np = (images_denorm.clamp(0, 1) * 255).byte().cpu().numpy()\n    # Permute from [B, C, H, W] to [B, H, W, C] for OpenCV\n    images_np = np.transpose(images_np, (0, 2, 3, 1))\n    \n    batch_morpho_features = []\n    \n    # 2. Loop through batch (on CPU)\n    for img_bgr in images_np: # Array is [B,H,W,C]\n        # PyTorch/PIL uses RGB, OpenCV uses BGR. We must convert B->C->H->W to H->W->C\n        # The permute already did this. The C channel is RGB.\n        img_hsv = cv2.cvtColor(img_bgr, cv2.COLOR_RGB2HSV)\n        \n        # 3. Create green mask\n        # These ranges cover most shades of green\n        lower_green = np.array([30, 40, 40])\n        upper_green = np.array([90, 255, 255])\n        binary_mask = cv2.inRange(img_hsv, lower_green, upper_green)\n        \n        # 4. Use skimage.measure.regionprops\n        props = measure.regionprops(binary_mask)\n        \n        if props:\n            # Use properties of the largest region\n            largest_prop = max(props, key=lambda p: p.area)\n            area = largest_prop.area\n            perimeter = largest_prop.perimeter\n            eccentricity = largest_prop.eccentricity\n            solidity = largest_prop.solidity\n            \n            # Normalize features\n            h, w = binary_mask.shape\n            area = area / (h * w) # % of image\n            perimeter = perimeter / (h * 2 + w * 2) # approx %\n            \n            # Handle potential NaNs from regionprops\n            if np.isnan(eccentricity): eccentricity = 0.0\n            if np.isnan(solidity): solidity = 0.0\n            \n        else:\n            # No regions found, return all zeros\n            area, perimeter, eccentricity, solidity = 0.0, 0.0, 0.0, 0.0\n            \n        batch_morpho_features.append([area, perimeter, eccentricity, solidity])\n\n    # 5. Convert back to tensor on the GPU\n    return torch.tensor(batch_morpho_features, dtype=torch.float32, device=DEVICE)\n\nprint(\"✓ 'get_real_morpho_features_hsv' function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:11.858715Z","iopub.execute_input":"2025-11-05T04:11:11.858938Z","iopub.status.idle":"2025-11-05T04:11:11.874742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Loop with Progress Tracking\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING (with REAL morphometric extraction)\")\nprint(\"=\"*60)\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'learning_rate': []\n}\n\nbest_val_acc = 0.0\npatience_counter = 0\nstart_time = datetime.now()\n\n# (The get_dummy_morpho function is no longer needed)\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_start = datetime.now()\n    \n    # ========== TRAINING PHASE ==========\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    \n    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n    \n    for batch_idx, (images, labels) in enumerate(train_pbar):\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        # ▼▼▼ THIS IS THE REAL CODE REPLACEMENT ▼▼▼\n        # Generate morphometric features on-the-fly\n        morpho_features = get_real_morpho_features_hsv(images)\n        # ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n        \n        # Forward pass\n        optimizer.zero_grad()\n        logits, _ = model(images, morpho_features)\n        loss = criterion(logits, labels)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        # Metrics\n        train_loss += loss.item()\n        preds = logits.argmax(dim=1)\n        train_correct += (preds == labels).sum().item()\n        train_total += labels.size(0)\n        \n        # Update progress bar\n        train_pbar.set_postfix({\n            'loss': f'{loss.item():.4f}',\n            'acc': f'{100.0 * train_correct / train_total:.2f}%'\n        })\n    \n    avg_train_loss = train_loss / len(train_loader)\n    train_acc = 100.0 * train_correct / train_total\n    \n    # ========== VALIDATION PHASE ==========\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]  \")\n    \n    with torch.no_grad():\n        for images, labels in val_pbar:\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            # ▼▼▼ THIS IS THE REAL CODE REPLACEMENT ▼▼▼\n            morpho_features = get_real_morpho_features_hsv(images)\n            # ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n            \n            logits, _ = model(images, morpho_features)\n            loss = criterion(logits, labels)\n            \n            val_loss += loss.item()\n            preds = logits.argmax(dim=1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n            \n            val_pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100.0 * val_correct / val_total:.2f}%'\n            })\n    \n    avg_val_loss = val_loss / len(val_loader)\n    val_acc = 100.0 * val_correct / val_total\n    \n    # Update learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    scheduler.step()\n    \n    # Save history\n    history['train_loss'].append(avg_train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(avg_val_loss)\n    history['val_acc'].append(val_acc)\n    history['learning_rate'].append(current_lr)\n    \n    # Epoch summary\n    epoch_time = (datetime.now() - epoch_start).total_seconds()\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} Summary:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n    print(f\"  LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': val_acc,\n            'train_acc': train_acc,\n        }, 'models/best_model.pth')\n        print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n    else:\n        patience_counter += 1\n        print(f\"  Patience: {patience_counter}/{PATIENCE}\")\n    \n    # Save periodic checkpoint\n    if (epoch + 1) % SAVE_EVERY == 0:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, f'models/checkpoint_epoch_{epoch+1}.pth')\n        print(f\"  ✓ Checkpoint saved\")\n    \n    # Early stopping\n    if patience_counter >= PATIENCE and not RUN_SMOKE_TEST:\n        print(f\"\\n⚠️  Early stopping triggered after {epoch+1} epochs\")\n        break\n    \n    print()\n\n# Save final model\ntorch.save({\n    'epoch': NUM_EPOCHS - 1,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n}, 'models/final_model.pth')\n\ntotal_time = (datetime.now() - start_time).total_seconds()\nprint(\"=\"*60)\nprint(f\"✓ Training completed in {total_time/60:.1f} minutes\")\nprint(f\"✓ Best validation accuracy: {best_val_acc:.2f}%\")\nprint(f\"✓ Models saved to models/\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:11:11.875413Z","iopub.execute_input":"2025-11-05T04:11:11.87564Z","iopub.status.idle":"2025-11-05T04:33:25.672743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Training History\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAVING TRAINING HISTORY\")\nprint(\"=\"*60)\n\n# Save as JSON\nhistory_json = {\n    'training_config': {\n        'num_epochs': NUM_EPOCHS,\n        'batch_size': BATCH_SIZE,\n        'learning_rate': LEARNING_RATE,\n        'weight_decay': WEIGHT_DECAY,\n        'optimizer': 'AdamW',\n        'scheduler': 'Warmup+CosineAnnealing',\n        'best_val_acc': best_val_acc,\n    },\n    'history': history\n}\n\nwith open('logs/training_history.json', 'w') as f:\n    json.dump(history_json, f, indent=2)\n\n# Save as CSV\nhistory_df = pd.DataFrame(history)\nhistory_df.to_csv('logs/training_log.csv', index=False)\n\nprint(f\"✓ Training history saved to logs/training_history.json\")\nprint(f\"✓ Training log saved to logs/training_log.csv\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:33:25.674195Z","iopub.execute_input":"2025-11-05T04:33:25.674484Z","iopub.status.idle":"2025-11-05T04:33:25.693106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training Curves\nprint(\"\\n\" + \"=\"*60)\nprint(\"PLOTTING TRAINING CURVES\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Loss curves\naxes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\naxes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=10)\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy curves\naxes[1].plot(history['train_acc'], label='Train Acc', linewidth=2)\naxes[1].plot(history['val_acc'], label='Val Acc', linewidth=2)\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Accuracy (%)', fontsize=12)\naxes[1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=10)\naxes[1].grid(True, alpha=0.3)\n\n# Learning rate schedule\naxes[2].plot(history['learning_rate'], linewidth=2, color='green')\naxes[2].set_xlabel('Epoch', fontsize=12)\naxes[2].set_ylabel('Learning Rate', fontsize=12)\naxes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\naxes[2].set_yscale('log')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('figures/training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Training curves saved to figures/training_curves.png\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:33:25.693886Z","iopub.execute_input":"2025-11-05T04:33:25.694067Z","iopub.status.idle":"2025-11-05T04:33:27.28417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Best Model for Evaluation\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING BEST MODEL FOR EVALUATION\")\nprint(\"=\"*60)\n\ncheckpoint = torch.load('models/best_model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f\"✓ Best model loaded (Epoch {checkpoint['epoch']+1})\")\nprint(f\"✓ Validation accuracy: {checkpoint['val_acc']:.2f}%\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:33:27.28507Z","iopub.execute_input":"2025-11-05T04:33:27.285432Z","iopub.status.idle":"2025-11-05T04:33:28.485663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comprehensive Evaluation on Test Set\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATING ON TEST SET\")\nprint(\"=\"*60)\n\nall_preds = []\nall_labels = []\nall_probs = []\n\ntest_pbar = tqdm(test_loader, desc=\"Testing\")\n\nwith torch.no_grad():\n    for images, labels in test_pbar:\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        morpho_features = get_real_morpho_features_hsv(images)\n        \n        logits, _ = model(images, morpho_features)\n        probs = torch.softmax(logits, dim=1)\n        preds = logits.argmax(dim=1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\nall_probs = np.array(all_probs)\n\n# Calculate metrics\ntest_acc = accuracy_score(all_labels, all_preds)\nprecision, recall, f1, support = precision_recall_fscore_support(\n    all_labels, all_preds, average='weighted', zero_division=0\n)\n\nprint(f\"\\n✓ Test Set Results:\")\nprint(f\"  Accuracy:  {test_acc*100:.2f}%\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall:    {recall:.4f}\")\nprint(f\"  F1-Score:  {f1:.4f}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:33:28.486532Z","iopub.execute_input":"2025-11-05T04:33:28.48682Z","iopub.status.idle":"2025-11-05T04:36:07.978711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Classification Report\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATING CLASSIFICATION REPORT\")\nprint(\"=\"*60)\n\n# Per-class metrics\nreport_dict = classification_report(\n    all_labels, all_preds,\n    target_names=PLANTVILLAGE_CLASSES,\n    output_dict=True,\n    zero_division=0\n)\n\n# Save detailed report\nwith open('tables/classification_report.json', 'w') as f:\n    json.dump(report_dict, f, indent=2)\n\n# Create summary table\nclass_metrics = []\nfor class_name in PLANTVILLAGE_CLASSES:\n    if class_name in report_dict:\n        metrics = report_dict[class_name]\n        class_metrics.append({\n            'Class': class_name,\n            'Precision': f\"{metrics['precision']:.3f}\",\n            'Recall': f\"{metrics['recall']:.3f}\",\n            'F1-Score': f\"{metrics['f1-score']:.3f}\",\n            'Support': int(metrics['support'])\n        })\n\nmetrics_df = pd.DataFrame(class_metrics)\nmetrics_df.to_csv('tables/per_class_metrics.csv', index=False)\n\n# Print top 10 classes\nprint(\"\\nTop 10 Classes by F1-Score:\")\nprint(metrics_df.head(10).to_string(index=False))\n\nprint(f\"\\n✓ Classification report saved to tables/classification_report.json\")\nprint(f\"✓ Per-class metrics saved to tables/per_class_metrics.csv\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:36:07.980439Z","iopub.execute_input":"2025-11-05T04:36:07.980721Z","iopub.status.idle":"2025-11-05T04:36:08.020422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Confusion Matrix\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATING CONFUSION MATRIX\")\nprint(\"=\"*60)\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Plot confusion matrix (38x38 - use smaller font)\nplt.figure(figsize=(20, 18))\nsns.heatmap(\n    cm, \n    annot=False,  # Too many classes for annotation\n    fmt='d',\n    cmap='Blues',\n    xticklabels=PLANTVILLAGE_CLASSES,\n    yticklabels=PLANTVILLAGE_CLASSES,\n    cbar_kws={'label': 'Count'}\n)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.ylabel('True Label', fontsize=12)\nplt.title('Confusion Matrix - Test Set (38 Classes)', fontsize=14, fontweight='bold')\nplt.xticks(rotation=90, ha='right', fontsize=8)\nplt.yticks(rotation=0, fontsize=8)\nplt.tight_layout()\nplt.savefig('figures/confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n# Save confusion matrix as CSV\ncm_df = pd.DataFrame(cm, index=PLANTVILLAGE_CLASSES, columns=PLANTVILLAGE_CLASSES)\ncm_df.to_csv('tables/confusion_matrix.csv')\n\nprint(f\"✓ Confusion matrix visualization saved to figures/confusion_matrix.png\")\nprint(f\"✓ Confusion matrix CSV saved to tables/confusion_matrix.csv\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:36:08.021328Z","iopub.execute_input":"2025-11-05T04:36:08.022023Z","iopub.status.idle":"2025-11-05T04:36:09.386704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# REPLACE the entire visualization cell with this\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmodel.eval()\nsample_batch = next(iter(test_loader))\n\n# --- Robust batch unpacking ---\nimages = None\nlabels = None\nmorpho = None\n\nif isinstance(sample_batch, dict):\n    images = sample_batch.get('image') or sample_batch.get('images') or sample_batch.get('img')\n    labels = sample_batch.get('label') or sample_batch.get('labels') or sample_batch.get('target')\n    morpho = sample_batch.get('morpho') or sample_batch.get('meta')\nelif isinstance(sample_batch, (list, tuple)):\n    if len(sample_batch) == 1:\n        images = sample_batch[0]\n    elif len(sample_batch) == 2:\n        images, labels = sample_batch\n    else:\n        images, labels = sample_batch[0], sample_batch[1]\n        morpho = sample_batch[2] if len(sample_batch) > 2 else None\nelse:\n    raise TypeError(f\"Unexpected batch type: {type(sample_batch)}. Print sample_batch to inspect.\")\n\nif images is None:\n    raise ValueError(\"Couldn't find images in the batch. Print `sample_batch` to inspect structure.\")\n\n# If morpho isn't provided, create a dummy (adjust dim if your model expects different)\nif morpho is None:\n    morpho = torch.randn(images.size(0), 4)  # change 4 -> your morpho dim\n\n# Move to device (model assumed to be on DEVICE)\nimages = images.to(DEVICE)\nmorpho = morpho.to(DEVICE)\nif isinstance(labels, torch.Tensor):\n    labels_np = labels.detach().cpu().numpy()\nelse:\n    labels_np = None\n\n# --- Forward pass ---\nwith torch.no_grad():\n    try:\n        outputs = model(images, morpho)   # try model signature with morpho\n    except TypeError:\n        outputs = model(images)          # fallback if model expects images only\n\n# If model returns a tuple/list like (logits, aux), take first element\nif isinstance(outputs, (list, tuple)):\n    outputs = outputs[0]\n\n# Ensure outputs is a tensor or convert safely\nif isinstance(outputs, torch.Tensor):\n    out_cpu = outputs.detach().cpu()\n    if out_cpu.dim() > 1:\n        preds = out_cpu.argmax(dim=1).numpy()\n    else:\n        preds = out_cpu.numpy()\nelse:\n    # fallback for numpy or other array-likes\n    preds = np.array(outputs).ravel()\n\n# --- Prepare images for display ---\nimages_cpu = images.detach().cpu()\nn_show = min(8, images_cpu.size(0))\nfig, axs = plt.subplots(1, n_show, figsize=(n_show*2.2, 2.2))\n\nfor i in range(n_show):\n    img = images_cpu[i]\n    # CHW -> HWC\n    if img.dim() == 3:\n        img_np = img.permute(1, 2, 0).numpy()\n        if img_np.shape[2] == 1:\n            img_np = img_np.squeeze(axis=2)\n    else:\n        img_np = img.numpy()\n\n    # If images were normalized (mean/std), undo that here before clipping\n    # Example (uncomment and set mean/std if needed):\n    # mean = np.array([0.485,0.456,0.406]); std = np.array([0.229,0.224,0.225])\n    # img_np = (img_np * std) + mean\n\n    # Normalize to displayable range\n    if img_np.max() > 1.0:\n        img_np = np.clip(img_np, 0, 255) / 255.0\n    else:\n        img_np = np.clip(img_np, 0, 1.0)\n\n    ax = axs[i] if n_show > 1 else axs\n    if img_np.ndim == 2:\n        ax.imshow(img_np, cmap='gray')\n    else:\n        ax.imshow(img_np)\n    title = f\"P:{int(preds[i])}\"\n    if labels_np is not None:\n        title += f\" / T:{int(labels_np[i])}\"\n    ax.set_title(title, fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:40:08.809382Z","iopub.execute_input":"2025-11-05T04:40:08.810193Z","iopub.status.idle":"2025-11-05T04:40:10.638357Z","shell.execute_reply.started":"2025-11-05T04:40:08.810163Z","shell.execute_reply":"2025-11-05T04:40:10.637384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Causal Knowledge Base: Disease -> Pathogen -> Treatment\n# Based on plant pathology literature and agricultural extension guides\n\nCAUSAL_RULES = {\n    'Apple___Apple_scab': {\n        'pathogen': 'Venturia inaequalis (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind-borne ascospores from overwintered leaf litter',\n        'treatments': [\n            'Apply fungicides (captan, myclobutanil) at green tip stage',\n            'Remove fallen leaves to reduce inoculum',\n            'Plant resistant cultivars (e.g., Liberty, Enterprise)',\n            'Maintain proper tree spacing for air circulation'\n        ],\n        'references': 'MacHardy (1996), Biggs & Miller (2001)'\n    },\n    'Apple___Black_rot': {\n        'pathogen': 'Botryosphaeria obtusa (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Spores spread by rain splash and wind',\n        'treatments': [\n            'Prune infected branches and cankers',\n            'Apply captan or thiophanate-methyl fungicides',\n            'Remove mummified fruits',\n            'Improve orchard sanitation'\n        ],\n        'references': 'Sutton (1990), Úrbez-Torres et al. (2012)'\n    },\n    'Apple___Cedar_apple_rust': {\n        'pathogen': 'Gymnosporangium juniperi-virginianae (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Alternates between apple and juniper hosts',\n        'treatments': [\n            'Apply myclobutanil or propiconazole fungicides',\n            'Remove nearby juniper trees (alternate host)',\n            'Plant resistant apple varieties',\n            'Apply treatments from pink bud to petal fall'\n        ],\n        'references': 'Aldwinckle (1990), Yoder et al. (2009)'\n    },\n    'Apple___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Continue routine monitoring',\n            'Maintain balanced fertilization',\n            'Ensure adequate irrigation',\n            'Practice preventive IPM strategies'\n        ],\n        'references': 'N/A'\n    },\n    'Blueberry___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Monitor for early disease symptoms',\n            'Maintain soil pH 4.5-5.5',\n            'Prune for air circulation',\n            'Apply preventive fungicides if conditions favor disease'\n        ],\n        'references': 'N/A'\n    },\n    'Cherry_(including_sour)___Powdery_mildew': {\n        'pathogen': 'Podosphaera clandestina (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind-borne conidia in warm, humid conditions',\n        'treatments': [\n            'Apply sulfur or myclobutanil fungicides',\n            'Prune to improve air circulation',\n            'Avoid overhead irrigation',\n            'Remove infected shoot tips'\n        ],\n        'references': 'Grove & Boal (1991), Xu et al. (2010)'\n    },\n    'Cherry_(including_sour)___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Continue monitoring for bacterial canker and brown rot',\n            'Maintain proper pruning practices',\n            'Ensure adequate nutrition',\n            'Practice preventive disease management'\n        ],\n        'references': 'N/A'\n    },\n    'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': {\n        'pathogen': 'Cercospora zeae-maydis (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Splash-dispersed conidia from crop residue',\n        'treatments': [\n            'Plant resistant hybrids',\n            'Apply strobilurin or triazole fungicides',\n            'Practice crop rotation (2-3 years)',\n            'Reduce surface residue through tillage'\n        ],\n        'references': 'Ward et al. (1999), Benson et al. (2015)'\n    },\n    'Corn_(maize)___Common_rust_': {\n        'pathogen': 'Puccinia sorghi (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind-borne urediniospores from southern regions',\n        'treatments': [\n            'Plant resistant hybrids with Rp genes',\n            'Apply triazole fungicides if severe',\n            'Monitor disease severity at silking stage',\n            'Generally does not require treatment in resistant varieties'\n        ],\n        'references': 'Hooker (1985), Pataky & Eastburn (1993)'\n    },\n    'Corn_(maize)___Northern_Leaf_Blight': {\n        'pathogen': 'Exserohilum turcicum (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind and rain-splashed conidia from residue',\n        'treatments': [\n            'Plant hybrids with Ht resistance genes',\n            'Apply strobilurin fungicides at V8-VT stages',\n            'Practice crop rotation',\n            'Bury or remove crop residue'\n        ],\n        'references': 'Welz & Geiger (2000), Nieuwoudt et al. (2018)'\n    },\n    'Corn_(maize)___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Scout regularly for disease symptoms',\n            'Maintain balanced fertilization',\n            'Ensure proper plant density',\n            'Practice integrated pest management'\n        ],\n        'references': 'N/A'\n    },\n    'Grape___Black_rot': {\n        'pathogen': 'Guignardia bidwellii (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Rain-splashed ascospores from mummified berries',\n        'treatments': [\n            'Apply mancozeb or myclobutanil fungicides',\n            'Remove mummified berries and infected leaves',\n            'Prune for air circulation',\n            'Apply treatments from bud break to 6 weeks post-bloom'\n        ],\n        'references': 'Hoffman et al. (2004), Wilcox (2005)'\n    },\n    'Grape___Esca_(Black_Measles)': {\n        'pathogen': 'Phaeomoniella chlamydospora, Phaeoacremonium spp. (fungi)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wound infection through pruning cuts',\n        'treatments': [\n            'No curative treatment available',\n            'Prune during dormancy to reduce infection risk',\n            'Apply wound protectants after pruning',\n            'Remove severely infected vines',\n            'Delay pruning until late winter'\n        ],\n        'references': 'Mugnai et al. (1999), Bertsch et al. (2013)'\n    },\n    'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': {\n        'pathogen': 'Pseudocercospora vitis (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Rain-splashed conidia in warm, humid conditions',\n        'treatments': [\n            'Apply copper-based fungicides or mancozeb',\n            'Improve canopy air circulation through pruning',\n            'Avoid overhead irrigation',\n            'Remove infected leaves'\n        ],\n        'references': 'Pscheidt & Pearson (1989), Gaforio et al. (2011)'\n    },\n    'Grape___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Continue monitoring for powdery mildew and downy mildew',\n            'Maintain proper canopy management',\n            'Scout for insect pests',\n            'Practice preventive fungicide applications if needed'\n        ],\n        'references': 'N/A'\n    },\n    'Orange___Haunglongbing_(Citrus_greening)': {\n        'pathogen': 'Candidatus Liberibacter asiaticus (bacterium)',\n        'pathogen_type': 'Bacterial',\n        'transmission': 'Asian citrus psyllid (Diaphorina citri) vector',\n        'treatments': [\n            'Remove infected trees to prevent spread',\n            'Control psyllid vectors with insecticides',\n            'Plant certified disease-free nursery stock',\n            'No cure available - focus on prevention',\n            'Apply foliar nutritional sprays to support tree health'\n        ],\n        'references': 'Bové (2006), Wang & Trivedi (2013)'\n    },\n    'Peach___Bacterial_spot': {\n        'pathogen': 'Xanthomonas arboricola pv. pruni (bacterium)',\n        'pathogen_type': 'Bacterial',\n        'transmission': 'Rain splash and wind-driven rain',\n        'treatments': [\n            'Apply copper-based bactericides',\n            'Plant resistant cultivars',\n            'Prune to improve air circulation',\n            'Apply treatments from shuck split to harvest',\n            'Avoid working in wet foliage'\n        ],\n        'references': 'Ritchie (1995), Stefani (2010)'\n    },\n    'Pepper,_bell___Bacterial_spot': {\n        'pathogen': 'Xanthomonas spp. (bacterium)',\n        'pathogen_type': 'Bacterial',\n        'transmission': 'Seed-borne, splash dispersal, mechanical transmission',\n        'treatments': [\n            'Use disease-free certified seed',\n            'Apply copper + mancozeb bactericides',\n            'Practice 3-year crop rotation',\n            'Remove and destroy infected plants',\n            'Avoid overhead irrigation'\n        ],\n        'references': 'Jones et al. (1986), Potnis et al. (2015)'\n    },\n    'Pepper,_bell___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Monitor for bacterial spot and phytophthora blight',\n            'Maintain proper spacing for air circulation',\n            'Ensure balanced fertilization',\n            'Scout regularly for insect pests'\n        ],\n        'references': 'N/A'\n    },\n    'Potato___Early_blight': {\n        'pathogen': 'Alternaria solani (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind and rain-splashed conidia from infected tissue',\n        'treatments': [\n            'Apply chlorothalonil or mancozeb fungicides',\n            'Practice 2-3 year crop rotation',\n            'Plant certified disease-free seed potatoes',\n            'Maintain adequate plant nutrition (especially nitrogen)',\n            'Destroy crop residue after harvest'\n        ],\n        'references': 'Rotem (1994), Leiminger & Hausladen (2012)'\n    },\n    'Potato___Late_blight': {\n        'pathogen': 'Phytophthora infestans (oomycete)',\n        'pathogen_type': 'Oomycete',\n        'transmission': 'Wind-dispersed sporangia in cool, wet conditions',\n        'treatments': [\n            'Apply mancozeb, chlorothalonil, or cymoxanil fungicides',\n            'Destroy volunteer potatoes and cull piles',\n            'Plant resistant varieties',\n            'Apply preventive fungicides before symptoms appear',\n            'Monitor weather conditions for disease-favorable periods'\n        ],\n        'references': 'Fry & Goodwin (1997), Haverkort et al. (2016)'\n    },\n    'Potato___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Continue monitoring for late blight and early blight',\n            'Maintain proper hilling to protect tubers',\n            'Scout for Colorado potato beetle',\n            'Ensure adequate irrigation and nutrition'\n        ],\n        'references': 'N/A'\n    },\n    'Raspberry___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Monitor for anthracnose and botrytis',\n            'Prune out old fruiting canes after harvest',\n            'Maintain proper row spacing',\n            'Practice preventive disease management'\n        ],\n        'references': 'N/A'\n    },\n    'Soybean___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Scout for sudden death syndrome and frogeye leaf spot',\n            'Practice crop rotation with non-host crops',\n            'Ensure proper drainage',\n            'Monitor for insect pests (soybean aphid, spider mites)'\n        ],\n        'references': 'N/A'\n    },\n    'Squash___Powdery_mildew': {\n        'pathogen': 'Podosphaera xanthii (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind-borne conidia in warm, dry conditions',\n        'treatments': [\n            'Apply sulfur or potassium bicarbonate fungicides',\n            'Plant resistant varieties',\n            'Improve air circulation through spacing',\n            'Remove heavily infected leaves',\n            'Apply preventive treatments before symptoms appear'\n        ],\n        'references': 'McGrath (2001), Pérez-García et al. (2009)'\n    },\n    'Strawberry___Leaf_scorch': {\n        'pathogen': 'Diplocarpon earlianum (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Splash-dispersed conidia during wet periods',\n        'treatments': [\n            'Apply captan or myclobutanil fungicides',\n            'Remove infected leaves',\n            'Improve air circulation through row spacing',\n            'Avoid overhead irrigation during fruiting',\n            'Plant resistant cultivars'\n        ],\n        'references': 'Maas (1998), Carisse et al. (2013)'\n    },\n    'Strawberry___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Monitor for botrytis, leaf scorch, and anthracnose',\n            'Maintain proper plant spacing',\n            'Remove old leaves after harvest',\n            'Practice integrated pest management'\n        ],\n        'references': 'N/A'\n    },\n    'Tomato___Bacterial_spot': {\n        'pathogen': 'Xanthomonas spp. (bacterium)',\n        'pathogen_type': 'Bacterial',\n        'transmission': 'Seed-borne, splash dispersal, mechanical transmission',\n        'treatments': [\n            'Use certified disease-free transplants',\n            'Apply copper + mancozeb bactericides',\n            'Practice 3-year crop rotation with non-solanaceous crops',\n            'Remove infected plants',\n            'Avoid overhead irrigation and working in wet fields'\n        ],\n        'references': 'Jones et al. (1998), Potnis et al. (2015)'\n    },\n    'Tomato___Early_blight': {\n        'pathogen': 'Alternaria solani (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Wind and rain-splashed conidia from infected tissue',\n        'treatments': [\n            'Apply chlorothalonil or mancozeb fungicides',\n            'Stake and prune plants for air circulation',\n            'Practice crop rotation',\n            'Remove lower infected leaves',\n            'Mulch to prevent soil splash'\n        ],\n        'references': 'Rotem (1994), Chaerani & Voorrips (2006)'\n    },\n    'Tomato___Late_blight': {\n        'pathogen': 'Phytophthora infestans (oomycete)',\n        'pathogen_type': 'Oomycete',\n        'transmission': 'Wind-dispersed sporangia in cool, wet conditions',\n        'treatments': [\n            'Apply mancozeb, chlorothalonil, or cymoxanil fungicides',\n            'Destroy infected plants immediately',\n            'Plant resistant varieties',\n            'Apply preventive fungicides before disease onset',\n            'Monitor weather for disease-favorable conditions'\n        ],\n        'references': 'Fry & Goodwin (1997), Foolad et al. (2008)'\n    },\n    'Tomato___Leaf_Mold': {\n        'pathogen': 'Passalora fulva (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Airborne conidia in high humidity (>85%)',\n        'treatments': [\n            'Improve greenhouse ventilation to reduce humidity',\n            'Apply chlorothalonil or copper fungicides',\n            'Plant resistant varieties with Cf genes',\n            'Space plants properly for air circulation',\n            'Avoid overhead irrigation'\n        ],\n        'references': 'Jones et al. (1997), Thomma et al. (2005)'\n    },\n    'Tomato___Septoria_leaf_spot': {\n        'pathogen': 'Septoria lycopersici (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Rain-splashed pycnidiospores from infected tissue',\n        'treatments': [\n            'Apply chlorothalonil or mancozeb fungicides',\n            'Remove infected lower leaves',\n            'Mulch to prevent soil splash',\n            'Practice crop rotation',\n            'Stake plants to improve air circulation'\n        ],\n        'references': 'Stevenson (1991), Pernezny et al. (2003)'\n    },\n    'Tomato___Spider_mites Two-spotted_spider_mite': {\n        'pathogen': 'Tetranychus urticae (arthropod pest)',\n        'pathogen_type': 'Arachnid',\n        'transmission': 'Wind dispersal, mechanical transfer on equipment',\n        'treatments': [\n            'Apply miticides (abamectin, bifenazate)',\n            'Release predatory mites (Phytoseiulus persimilis)',\n            'Increase humidity to suppress populations',\n            'Remove heavily infested plants',\n            'Avoid broad-spectrum insecticides that kill natural enemies'\n        ],\n        'references': 'Helle & Sabelis (1985), Van Leeuwen et al. (2015)'\n    },\n    'Tomato___Target_Spot': {\n        'pathogen': 'Corynespora cassiicola (fungus)',\n        'pathogen_type': 'Fungal',\n        'transmission': 'Rain splash and wind-dispersed conidia',\n        'treatments': [\n            'Apply chlorothalonil or azoxystrobin fungicides',\n            'Improve air circulation through staking and pruning',\n            'Practice crop rotation',\n            'Remove infected leaves',\n            'Avoid overhead irrigation'\n        ],\n        'references': 'Pernezny et al. (2002), Dixon et al. (2009)'\n    },\n    'Tomato___Tomato_Yellow_Leaf_Curl_Virus': {\n        'pathogen': 'Tomato yellow leaf curl virus (begomovirus)',\n        'pathogen_type': 'Viral',\n        'transmission': 'Whitefly (Bemisia tabaci) vector',\n        'treatments': [\n            'Control whitefly vectors with insecticides (imidacloprid)',\n            'Use reflective mulches to repel whiteflies',\n            'Plant resistant varieties with Ty genes',\n            'Remove infected plants immediately',\n            'Use insect-proof screens in greenhouses'\n        ],\n        'references': 'Moriones & Navas-Castillo (2000), Lapidot & Friedmann (2002)'\n    },\n    'Tomato___Tomato_mosaic_virus': {\n        'pathogen': 'Tomato mosaic virus (tobamovirus)',\n        'pathogen_type': 'Viral',\n        'transmission': 'Mechanical transmission, seed-borne, contact',\n        'treatments': [\n            'Use virus-free certified seed and transplants',\n            'Plant resistant varieties with Tm genes',\n            'Sanitize tools and hands between plants',\n            'Remove and destroy infected plants',\n            'Control aphid vectors if present'\n        ],\n        'references': 'Broadbent (1976), Lewandowski & Dawson (2000)'\n    },\n    'Tomato___healthy': {\n        'pathogen': 'None detected',\n        'pathogen_type': 'N/A',\n        'transmission': 'N/A',\n        'treatments': [\n            'Continue monitoring for early blight, late blight, and bacterial diseases',\n            'Maintain balanced fertilization (avoid excess nitrogen)',\n            'Ensure proper irrigation management',\n            'Scout regularly for insect pests and viruses'\n        ],\n        'references': 'N/A'\n    }\n}\n\nprint(\"=\"*60)\nprint(\"CAUSAL KNOWLEDGE BASE INITIALIZED\")\nprint(\"=\"*60)\nprint(f\"Total disease classes mapped: {len(CAUSAL_RULES)}\")\nprint(f\"Pathogen types: Fungal, Bacterial, Oomycete, Viral, Arachnid\")\nprint(f\"\\nSample entry: {list(CAUSAL_RULES.keys())[0]}\")\nprint(f\"  Pathogen: {CAUSAL_RULES[list(CAUSAL_RULES.keys())[0]]['pathogen']}\")\nprint(f\"  Treatments: {len(CAUSAL_RULES[list(CAUSAL_RULES.keys())[0]]['treatments'])} recommendations\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:40:26.437252Z","iopub.execute_input":"2025-11-05T04:40:26.437957Z","iopub.status.idle":"2025-11-05T04:40:26.462071Z","shell.execute_reply.started":"2025-11-05T04:40:26.437934Z","shell.execute_reply":"2025-11-05T04:40:26.461256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def causal_inference(predicted_class, confidence, causal_rules=CAUSAL_RULES):\n    \"\"\"\n    Perform causal inference to map disease prediction to pathogen and treatments.\n    \n    Args:\n        predicted_class (str): Predicted disease class name\n        confidence (float): Model confidence score (0-1)\n        causal_rules (dict): Knowledge base mapping diseases to pathogens/treatments\n    \n    Returns:\n        dict: Causal analysis including pathogen, transmission, treatments\n    \"\"\"\n    # Check if class exists in knowledge base\n    if predicted_class not in causal_rules:\n        return {\n            'disease': predicted_class,\n            'confidence': confidence,\n            'status': 'Unknown disease class',\n            'pathogen': 'Not in knowledge base',\n            'pathogen_type': 'N/A',\n            'transmission': 'N/A',\n            'treatments': ['Consult local agricultural extension service'],\n            'references': 'N/A'\n        }\n    \n    # Retrieve causal information\n    causal_info = causal_rules[predicted_class]\n    \n    # Construct response\n    result = {\n        'disease': predicted_class,\n        'confidence': confidence,\n        'status': 'Healthy' if 'healthy' in predicted_class.lower() else 'Disease detected',\n        'pathogen': causal_info['pathogen'],\n        'pathogen_type': causal_info['pathogen_type'],\n        'transmission': causal_info['transmission'],\n        'treatments': causal_info['treatments'],\n        'references': causal_info['references'],\n        'confidence_threshold': 0.7,\n        'action_recommended': confidence >= 0.7  # Only recommend action if high confidence\n    }\n    \n    return result\n\n\ndef format_causal_report(causal_result):\n    \"\"\"\n    Format causal inference result as a human-readable report.\n    \n    Args:\n        causal_result (dict): Output from causal_inference function\n    \n    Returns:\n        str: Formatted report string\n    \"\"\"\n    report = []\n    report.append(\"=\"*70)\n    report.append(\"CAUSAL INFERENCE REPORT\")\n    report.append(\"=\"*70)\n    report.append(f\"Disease: {causal_result['disease']}\")\n    report.append(f\"Status: {causal_result['status']}\")\n    report.append(f\"Model Confidence: {causal_result['confidence']:.2%}\")\n    report.append(f\"Action Recommended: {'YES' if causal_result['action_recommended'] else 'NO (Low confidence - verify manually)'}\")\n    report.append(\"-\"*70)\n    report.append(f\"Causative Agent: {causal_result['pathogen']}\")\n    report.append(f\"Pathogen Type: {causal_result['pathogen_type']}\")\n    report.append(f\"Transmission: {causal_result['transmission']}\")\n    report.append(\"-\"*70)\n    report.append(\"Recommended Treatments:\")\n    for i, treatment in enumerate(causal_result['treatments'], 1):\n        report.append(f\"  {i}. {treatment}\")\n    report.append(\"-\"*70)\n    report.append(f\"References: {causal_result['references']}\")\n    report.append(\"=\"*70)\n    \n    return \"\\n\".join(report)\n\n\n# Test the causal inference function\nprint(\"Testing causal inference function...\\n\")\n\n# Test case 1: Disease with high confidence\ntest_prediction_1 = 'Tomato___Late_blight'\ntest_confidence_1 = 0.95\nresult_1 = causal_inference(test_prediction_1, test_confidence_1)\nprint(format_causal_report(result_1))\n\nprint(\"\\n\\n\")\n\n# Test case 2: Healthy plant\ntest_prediction_2 = 'Tomato___healthy'\ntest_confidence_2 = 0.88\nresult_2 = causal_inference(test_prediction_2, test_confidence_2)\nprint(format_causal_report(result_2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:40:26.46543Z","iopub.execute_input":"2025-11-05T04:40:26.465821Z","iopub.status.idle":"2025-11-05T04:40:26.482271Z","shell.execute_reply.started":"2025-11-05T04:40:26.465803Z","shell.execute_reply":"2025-11-05T04:40:26.481531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Robust causal inference loop (replace your existing cell with this)\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nprint(\"=\"*60)\nprint(\"APPLYING CAUSAL ENGINE TO TEST SET PREDICTIONS\")\nprint(\"=\"*60)\n\nmodel.eval()\nall_causal_results = []\n\n# set this to the dim of morpho features your model expects\nMORPHO_DIM = 4\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Causal Inference\")):\n        try:\n            # --- Robust unpacking ---\n            images = None\n            labels = None\n            morpho = None\n\n            # dict-like\n            if isinstance(batch, dict):\n                images = batch.get('image') or batch.get('images') or batch.get('img')\n                labels = batch.get('label') or batch.get('labels') or batch.get('target')\n                morpho = batch.get('morpho') or batch.get('meta') or batch.get('morpho_features')\n\n            # list/tuple-like\n            elif isinstance(batch, (list, tuple)):\n                if len(batch) == 1:\n                    images = batch[0]\n                elif len(batch) == 2:\n                    images, labels = batch\n                else:\n                    # common: (images, labels, meta)\n                    images, labels = batch[0], batch[1]\n                    morpho = batch[2] if len(batch) > 2 else None\n            else:\n                # unexpected type: try to index like a sequence\n                try:\n                    images = batch[0]\n                except Exception:\n                    raise TypeError(f\"Unrecognized batch type: {type(batch)}\")\n\n            if images is None:\n                print(f\"[WARN] Couldn't find images in batch {batch_idx}. Skipping batch.\")\n                continue\n\n            # Convert numpy arrays to tensors if necessary\n            if isinstance(images, np.ndarray):\n                images = torch.from_numpy(images)\n\n            # move images to DEVICE\n            images = images.to(DEVICE)\n\n            # labels -> numpy array of integer class indices\n            if labels is None:\n                # no labels provided: create placeholder\n                labels_np = np.array([-1] * images.size(0))\n            else:\n                if isinstance(labels, torch.Tensor):\n                    if labels.dim() > 1:   # e.g., one-hot\n                        labels = labels.argmax(dim=1)\n                    labels_np = labels.detach().cpu().numpy()\n                elif isinstance(labels, np.ndarray):\n                    labels_np = labels\n                elif isinstance(labels, (list, tuple)):\n                    labels_np = np.array(labels)\n                else:\n                    # try to coerce\n                    labels_np = np.array(labels)\n\n            # morpho: if provided, else create dummy\n            if morpho is None:\n                morpho = torch.randn(images.size(0), MORPHO_DIM).to(DEVICE)\n            else:\n                if isinstance(morpho, np.ndarray):\n                    morpho = torch.from_numpy(morpho).to(DEVICE)\n                elif isinstance(morpho, torch.Tensor):\n                    morpho = morpho.to(DEVICE)\n                else:\n                    # fallback: create dummy\n                    morpho = torch.randn(images.size(0), MORPHO_DIM).to(DEVICE)\n\n            # --- Forward pass (handle different model signatures) ---\n            try:\n                outputs = model(images, morpho)\n            except TypeError:\n                # model might accept images only\n                outputs = model(images)\n\n            # if model returns (logits, aux), take first element\n            if isinstance(outputs, (list, tuple)):\n                outputs = outputs[0]\n\n            # ensure outputs is tensor and on CPU for numpy ops\n            if not isinstance(outputs, torch.Tensor):\n                outputs = torch.tensor(outputs)\n\n            outputs_cpu = outputs.detach().cpu()\n\n            # --- Probabilities and predictions ---\n            if outputs_cpu.dim() == 1:\n                # single-dim output (regression or single logit) -> treat as is\n                probs = outputs_cpu\n                # no preds concept; fallback to threshold 0.5\n                preds = (probs > 0.5).long()\n                confidences = probs.abs()\n            else:\n                probs = torch.softmax(outputs_cpu, dim=1)\n                confidences, preds = torch.max(probs, dim=1)\n\n            # --- Iterate samples in batch ---\n            batch_size = images.size(0)\n            for i in range(batch_size):\n                # safe extraction of numeric labels/preds\n                pred_idx = int(preds[i].item()) if isinstance(preds[i], (torch.Tensor, np.generic)) else int(preds[i])\n                conf = float(confidences[i].item()) if isinstance(confidences[i], (torch.Tensor, np.generic)) else float(confidences[i])\n\n                # handle label - may be -1 if absent\n                try:\n                    true_idx = int(labels_np[i])\n                except Exception:\n                    true_idx = -1\n\n                # map to class names if PLANTVILLAGE_CLASSES exists and index is valid\n                try:\n                    pred_class = PLANTVILLAGE_CLASSES[pred_idx]\n                except Exception:\n                    pred_class = str(pred_idx)\n\n                try:\n                    true_class = PLANTVILLAGE_CLASSES[true_idx] if true_idx >= 0 else None\n                except Exception:\n                    true_class = str(true_idx)\n\n                # Run your causal engine (assumes causal_inference exists)\n                causal_result = causal_inference(pred_class, conf)\n\n                # enrich and append\n                causal_result.update({\n                    'pred_idx': pred_idx,\n                    'true_idx': true_idx,\n                    'pred_class': pred_class,\n                    'true_class': true_class,\n                    'confidence': conf,\n                    'correct_prediction': (true_idx >= 0 and pred_idx == true_idx),\n                    # add any metadata you want (batch_idx, sample_idx)\n                    'batch_idx': batch_idx,\n                    'sample_in_batch': i\n                })\n\n                all_causal_results.append(causal_result)\n\n        except Exception as e:\n            # don't crash the whole loop; log and continue\n            print(f\"[ERROR] processing batch {batch_idx}: {e}\")\n            continue\n\nprint(f\"\\n✓ Causal inference completed for {len(all_causal_results)} test samples\")\n\n# Save results (ensure dir exists)\nimport os\nos.makedirs('tables', exist_ok=True)\ncausal_df = pd.DataFrame(all_causal_results)\ncausal_df.to_csv('tables/causal_inference_results.csv', index=False)\nprint(f\"✓ Causal results saved to tables/causal_inference_results.csv\")\n\n# --- Summary statistics (safe computations) ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"CAUSAL INFERENCE SUMMARY\")\nprint(\"=\"*60)\ntotal = len(all_causal_results)\nprint(f\"Total predictions: {total}\")\nif total:\n    high_conf = sum(1 for r in all_causal_results if r.get('action_recommended'))\n    disease_cases = sum(1 for r in all_causal_results if r.get('status') == 'Disease detected')\n    healthy_cases = sum(1 for r in all_causal_results if r.get('status') == 'Healthy')\n\n    print(f\"High confidence predictions (action recommended): {high_conf}\")\n    print(f\"Disease cases detected: {disease_cases}\")\n    print(f\"Healthy cases detected: {healthy_cases}\")\n\n    # Pathogen type distribution\n    pathogen_counts = {}\n    for result in all_causal_results:\n        ptype = result.get('pathogen_type', 'Unknown')\n        pathogen_counts[ptype] = pathogen_counts.get(ptype, 0) + 1\n\n    print(f\"\\nPathogen Type Distribution:\")\n    for ptype, count in sorted(pathogen_counts.items(), key=lambda x: x[1], reverse=True):\n        pct = count / total * 100\n        print(f\"  {ptype}: {count} ({pct:.1f}%)\")\nelse:\n    print(\"No causal results to summarize.\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:43:04.272035Z","iopub.execute_input":"2025-11-05T04:43:04.272686Z","iopub.status.idle":"2025-11-05T04:43:55.324926Z","shell.execute_reply.started":"2025-11-05T04:43:04.272652Z","shell.execute_reply":"2025-11-05T04:43:55.323795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate sample treatment reports for diverse disease cases\nprint(\"=\"*60)\nprint(\"GENERATING SAMPLE TREATMENT REPORTS\")\nprint(\"=\"*60)\n\n# Select diverse sample predictions (different pathogen types)\nselected_samples = []\npathogen_types_covered = set()\n\nfor result in all_causal_results:\n    # Select high-confidence disease cases covering different pathogen types\n    if (result['status'] == 'Disease detected' and \n        result['action_recommended'] and \n        result['pathogen_type'] not in pathogen_types_covered and\n        result['pathogen_type'] != 'N/A'):\n        \n        selected_samples.append(result)\n        pathogen_types_covered.add(result['pathogen_type'])\n        \n        if len(selected_samples) >= 5:  # Limit to 5 diverse examples\n            break\n\n# Generate detailed reports\nsample_reports = []\nfor idx, sample in enumerate(selected_samples, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"SAMPLE TREATMENT REPORT #{idx}\")\n    print('='*70)\n    \n    report_text = format_causal_report(sample)\n    print(report_text)\n    \n    # Add to collection\n    sample_reports.append({\n        'report_id': idx,\n        'disease': sample['disease'],\n        'pathogen_type': sample['pathogen_type'],\n        'confidence': sample['confidence'],\n        'report': report_text\n    })\n\n# Save sample reports to file\nwith open('tables/sample_treatment_reports.txt', 'w') as f:\n    for report_data in sample_reports:\n        f.write(f\"\\n{'='*70}\\n\")\n        f.write(f\"SAMPLE TREATMENT REPORT #{report_data['report_id']}\\n\")\n        f.write(f\"{'='*70}\\n\")\n        f.write(report_data['report'])\n        f.write(\"\\n\\n\")\n\nprint(f\"\\n\\n✓ Generated {len(sample_reports)} sample treatment reports\")\nprint(f\"✓ Reports saved to tables/sample_treatment_reports.txt\")\nprint(f\"✓ Pathogen types covered: {', '.join(pathogen_types_covered)}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:44:03.73951Z","iopub.execute_input":"2025-11-05T04:44:03.740124Z","iopub.status.idle":"2025-11-05T04:44:03.750786Z","shell.execute_reply.started":"2025-11-05T04:44:03.740087Z","shell.execute_reply":"2025-11-05T04:44:03.75004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:03:27.50777Z","iopub.execute_input":"2025-11-05T05:03:27.508055Z","iopub.status.idle":"2025-11-05T05:03:27.512341Z","shell.execute_reply.started":"2025-11-05T05:03:27.508036Z","shell.execute_reply":"2025-11-05T05:03:27.511477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Patent Results Appendix\nprint(\"=\"*60)\nprint(\"GENERATING PATENT RESULTS APPENDIX\")\nprint(\"=\"*60)\n\nappendix_content = []\n\n# Header\nappendix_content.append(\"# PATENT RESULTS APPENDIX\")\nappendix_content.append(\"# Hybrid Multi-Branch Plant Disease Diagnosis System\")\nappendix_content.append(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nappendix_content.append(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# Section 1: System Configuration\nappendix_content.append(\"## 1. SYSTEM CONFIGURATION\\n\")\nappendix_content.append(\"### 1.1 Hardware & Environment\")\nappendix_content.append(f\"- Device: {DEVICE}\")\nappendix_content.append(f\"- GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    appendix_content.append(f\"- GPU Name: {torch.cuda.get_device_name(0)}\")\n    appendix_content.append(f\"- GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nappendix_content.append(f\"- Random Seed: {SEED}\")\nappendix_content.append(f\"- PyTorch Version: {torch.__version__}\")\n\nappendix_content.append(\"\\n### 1.2 Dataset Configuration\")\nappendix_content.append(f\"- Dataset: PlantVillage\")\nappendix_content.append(f\"- Total Classes: {NUM_CLASSES}\")\nappendix_content.append(f\"- Image Resolution: 224×224 pixels\")\nappendix_content.append(f\"- Train/Val/Test Split: 70%/15%/15%\")\nif 'train_images' in locals():\n    appendix_content.append(f\"- Training Samples: {len(train_images)}\")\n    appendix_content.append(f\"- Validation Samples: {len(val_images)}\")\n    appendix_content.append(f\"- Test Samples: {len(test_images)}\")\n\n# Section 2: Model Architecture\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 2. MODEL ARCHITECTURE\\n\")\nappendix_content.append(\"### 2.1 Hybrid Multi-Branch Classifier\")\nappendix_content.append(\"- **Branch 1 (Vision Transformer)**: facebook/vit-mae-base\")\nappendix_content.append(\"  - Pre-trained with Masked Autoencoding\")\nappendix_content.append(\"  - Feature dimension: 768 → 512 (projected)\")\nappendix_content.append(\"- **Branch 2 (Convolutional)**: EfficientNet-B0\")\nappendix_content.append(\"  - Efficient scaling with compound coefficients\")\nappendix_content.append(\"  - Feature dimension: 1280 → 512 (projected)\")\nappendix_content.append(\"- **Branch 3 (Morphometric)**: Custom MLP\")\nappendix_content.append(\"  - Input: 4 morphometric features (area, perimeter, eccentricity, solidity)\")\nappendix_content.append(\"  - Architecture: 4 → 64 → 512\")\nappendix_content.append(\"- **Fusion Strategy**: Concatenation (1536-dim) → 1024 → 512 → 38 classes\")\nappendix_content.append(f\"- **Total Parameters**: {sum(p.numel() for p in model.parameters()):,}\")\nappendix_content.append(f\"- **Trainable Parameters**: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# Section 3: Training Hyperparameters\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 3. TRAINING HYPERPARAMETERS\\n\")\nappendix_content.append(f\"- Optimizer: AdamW\")\nappendix_content.append(f\"- Initial Learning Rate: {LEARNING_RATE}\")\nappendix_content.append(f\"- Weight Decay: {WEIGHT_DECAY}\")\nappendix_content.append(f\"- Batch Size: {BATCH_SIZE}\")\nappendix_content.append(f\"- Number of Epochs: {NUM_EPOCHS}\")\nappendix_content.append(f\"- Gradient Clipping: max_norm=1.0\")\nappendix_content.append(f\"- Learning Rate Schedule: Linear Warmup (10%) + Cosine Annealing\")\nappendix_content.append(f\"- Early Stopping Patience: 5 epochs\")\nappendix_content.append(f\"- Loss Function: CrossEntropyLoss\")\n\n# Section 4: Augmentation Strategy\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 4. AUGMENTATION STRATEGY\\n\")\nappendix_content.append(\"### 4.1 Physics-Inspired Augmentations\")\nappendix_content.append(\"- Spectral Jitter (±15% RGB perturbation)\")\nappendix_content.append(\"- Dust Overlay (50-150 particles)\")\nappendix_content.append(\"- Water Droplets (10-25 droplets)\")\nappendix_content.append(\"- Atmospheric Haze/Fog\")\nappendix_content.append(\"- Dynamic Shadow Casting\")\nappendix_content.append(\"- Shot Noise & Gaussian Noise\")\nappendix_content.append(\"\\n### 4.2 Standard Augmentations\")\nappendix_content.append(\"- Random Horizontal/Vertical Flip\")\nappendix_content.append(\"- Random Rotation (±45°)\")\nappendix_content.append(\"- Random Brightness/Contrast\")\nappendix_content.append(\"- Coarse Dropout\")\nappendix_content.append(\"- ImageNet Normalization\")\n\n# Section 5: Performance Metrics\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 5. FINAL PERFORMANCE METRICS\\n\")\n\n# Load training history if available\nif os.path.exists('logs/training_history.json'):\n    with open('logs/training_history.json', 'r') as f:\n        history = json.load(f)\n    \n    appendix_content.append(\"### 5.1 Training History\")\n    appendix_content.append(f\"- Total Epochs Trained: {len(history['history']['train_loss'])}\")\n    appendix_content.append(f\"- Best Validation Accuracy: {max(history['history']['val_acc']):.4f}\")\n    appendix_content.append(f\"- Best Epoch: {history['history']['val_acc'].index(max(history['history']['val_acc'])) + 1}\")\n    appendix_content.append(f\"- Final Training Loss: {history['history']['train_loss'][-1]:.4f}\")\n    appendix_content.append(f\"- Final Validation Loss: {history['history']['val_loss'][-1]:.4f}\")\n\nappendix_content.append(\"\\n### 5.2 Test Set Performance\")\nif 'all_labels' in locals() and 'all_preds' in locals():\n    test_acc = accuracy_score(all_labels, all_preds)\n    test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n    \n    appendix_content.append(f\"- Test Accuracy: {test_acc:.4f}\")\n    appendix_content.append(f\"- Weighted Precision: {test_precision:.4f}\")\n    appendix_content.append(f\"- Weighted Recall: {test_recall:.4f}\")\n    appendix_content.append(f\"- Weighted F1-Score: {test_f1:.4f}\")\n\n# Section 6: Sample Predictions\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 6. TOP 5 SAMPLE PREDICTIONS\\n\")\n\n# Get 5 high-confidence correct predictions\nif 'all_causal_results' in locals():\n    high_conf_correct = [r for r in all_causal_results if r['correct_prediction'] and r['confidence'] >= 0.9]\n    top_samples = sorted(high_conf_correct, key=lambda x: x['confidence'], reverse=True)[:5]\n    \n    for idx, sample in enumerate(top_samples, 1):\n        appendix_content.append(f\"\\n### Sample {idx}\")\n        appendix_content.append(f\"- Disease: {sample['disease']}\")\n        appendix_content.append(f\"- Confidence: {sample['confidence']:.4f}\")\n        appendix_content.append(f\"- Pathogen: {sample['pathogen']}\")\n        appendix_content.append(f\"- Pathogen Type: {sample['pathogen_type']}\")\n        appendix_content.append(f\"- Treatment: {sample['treatments'][0]}\")\n\n# Section 7: Artifact Inventory\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 7. ARTIFACT INVENTORY\\n\")\nappendix_content.append(\"### 7.1 Model Checkpoints\")\nappendix_content.append(\"- `models/best_model.pth` - Best validation accuracy checkpoint\")\nappendix_content.append(\"- `models/final_model.pth` - Final epoch checkpoint\")\n\nappendix_content.append(\"\\n### 7.2 Visualizations (figures/)\")\nfigures_list = [\n    \"augmentation_examples.png - Physics-inspired augmentation showcase\",\n    \"segmentation_examples.png - SegFormer-B0 segmentation results\",\n    \"morphometric_examples.png - Morphometric feature extraction\",\n    \"architecture_diagram.png - Hybrid classifier architecture\",\n    \"training_curves.png - Training/validation loss and accuracy\",\n    \"confusion_matrix.png - 38×38 confusion matrix heatmap\",\n    \"sample_predictions.png - Test set prediction examples\"\n]\nfor fig in figures_list:\n    appendix_content.append(f\"- {fig}\")\n\nappendix_content.append(\"\\n### 7.3 Data Tables (tables/)\")\ntables_list = [\n    \"classification_report.json - Per-class precision/recall/F1\",\n    \"per_class_metrics.csv - Classification metrics table\",\n    \"confusion_matrix.csv - Confusion matrix data\",\n    \"causal_inference_results.csv - Full causal analysis results\",\n    \"sample_treatment_reports.txt - Example treatment recommendations\"\n]\nfor table in tables_list:\n    appendix_content.append(f\"- {table}\")\n\nappendix_content.append(\"\\n### 7.4 Logs & Metadata (logs/)\")\nlogs_list = [\n    \"training_history.json - Epoch-wise metrics\",\n    \"training_log.csv - Training log in CSV format\",\n    \"environment.txt - Python package versions\",\n    \"sources.txt - Dataset sources and citations\"\n]\nfor log in logs_list:\n    appendix_content.append(f\"- {log}\")\n\n# Section 8: Patent Claims Summary\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## 8. NOVEL CONTRIBUTIONS FOR PATENT CLAIMS\\n\")\nappendix_content.append(\"1. **Hybrid Multi-Branch Architecture**: Combines self-supervised ViT, efficient CNN, and morphometric features\")\nappendix_content.append(\"2. **Physics-Inspired Augmentations**: 15+ domain-specific transformations mimicking field conditions\")\nappendix_content.append(\"3. **Causal Inference Engine**: Rule-based pathogen mapping with confidence thresholding\")\nappendix_content.append(\"4. **End-to-End Explainability**: From raw image to treatment recommendation with evidence trail\")\nappendix_content.append(\"5. **Multi-Pathogen Coverage**: Handles fungal, bacterial, viral, oomycete, and pest etiologies\")\n\nappendix_content.append(\"\\n\" + \"=\"*70)\nappendix_content.append(\"\\n## END OF APPENDIX\")\nappendix_content.append(\"=\"*70)\n\n# Save appendix\nappendix_text = \"\\n\".join(appendix_content)\nwith open('appendix.md', 'w') as f:\n    f.write(appendix_text)\n\nprint(\"✓ Patent results appendix generated: appendix.md\")\nprint(f\"✓ Document length: {len(appendix_text)} characters\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:04:49.376976Z","iopub.execute_input":"2025-11-05T05:04:49.377536Z","iopub.status.idle":"2025-11-05T05:04:49.419042Z","shell.execute_reply.started":"2025-11-05T05:04:49.377512Z","shell.execute_reply":"2025-11-05T05:04:49.418339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Package all artifacts into final_results.zip\nimport zipfile\nimport glob\n\nprint(\"=\"*60)\nprint(\"PACKAGING FINAL RESULTS\")\nprint(\"=\"*60)\n\n# Create zip archive\nzip_filename = 'final_results.zip'\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    \n    # Add model checkpoints\n    print(\"\\nAdding model checkpoints...\")\n    for model_file in glob.glob('models/*.pth'):\n        if os.path.exists(model_file):\n            zipf.write(model_file)\n            print(f\"  ✓ {model_file}\")\n    \n    # Add figures\n    print(\"\\nAdding visualizations...\")\n    for fig_file in glob.glob('figures/*.png'):\n        if os.path.exists(fig_file):\n            zipf.write(fig_file)\n            print(f\"  ✓ {fig_file}\")\n    \n    # Add tables\n    print(\"\\nAdding data tables...\")\n    for table_file in glob.glob('tables/*'):\n        if os.path.exists(table_file) and os.path.isfile(table_file):\n            zipf.write(table_file)\n            print(f\"  ✓ {table_file}\")\n    \n    # Add logs\n    print(\"\\nAdding logs and metadata...\")\n    for log_file in glob.glob('logs/*'):\n        if os.path.exists(log_file) and os.path.isfile(log_file):\n            zipf.write(log_file)\n            print(f\"  ✓ {log_file}\")\n    \n    # Add appendix\n    if os.path.exists('appendix.md'):\n        zipf.write('appendix.md')\n        print(f\"  ✓ appendix.md\")\n    \n    # Add environment info\n    if os.path.exists('environment.txt'):\n        zipf.write('environment.txt')\n        print(f\"  ✓ environment.txt\")\n    \n    # Add dataset sources\n    if os.path.exists('sources.txt'):\n        zipf.write('sources.txt')\n        print(f\"  ✓ sources.txt\")\n\n# Get zip file size\nzip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # Convert to MB\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"✓ Final results packaged: {zip_filename}\")\nprint(f\"✓ Archive size: {zip_size:.2f} MB\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:05:52.039551Z","iopub.execute_input":"2025-11-05T05:05:52.039834Z","iopub.status.idle":"2025-11-05T05:07:39.624435Z","shell.execute_reply.started":"2025-11-05T05:05:52.039816Z","shell.execute_reply":"2025-11-05T05:07:39.623685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final Verification Checklist\nprint(\"\\n\" + \"=\"*70)\nprint(\" \"*20 + \"FINAL VERIFICATION CHECKLIST\")\nprint(\"=\"*70 + \"\\n\")\n\nchecklist = []\n\n# Category 1: Environment Setup\nprint(\"📋 CATEGORY 1: ENVIRONMENT SETUP\")\nchecks = [\n    (\"GPU Available\", torch.cuda.is_available(), \"GPU detection\"),\n    (\"Random Seed Set\", SEED == 42, f\"Seed = {SEED}\"),\n    (\"Directories Created\", all(os.path.exists(d) for d in ['figures', 'tables', 'models', 'logs', 'data']), \"Output directories\"),\n]\nfor name, status, detail in checks:\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {name}: {detail}\")\n    checklist.append({'category': 'Environment', 'check': name, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 2: Dataset\nprint(\"\\n📋 CATEGORY 2: DATASET\")\nchecks = [\n    (\"Dataset Loaded\", 'train_images' in locals(), \"Train/val/test splits created\"),\n    (\"Class Mapping\", NUM_CLASSES == 38, f\"{NUM_CLASSES} classes\"),\n    (\"Data Loaders\", 'train_loader' in locals(), \"PyTorch DataLoaders initialized\"),\n]\nfor name, status, detail in checks:\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {name}: {detail}\")\n    checklist.append({'category': 'Dataset', 'check': name, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 3: Model Architecture\nprint(\"\\n📋 CATEGORY 3: MODEL ARCHITECTURE\")\nchecks = [\n    (\"Model Initialized\", 'model' in locals(), \"HybridClassifier created\"),\n    (\"ViT Branch\", hasattr(model, 'vit'), \"Vision Transformer branch\"),\n    (\"CNN Branch\", hasattr(model, 'efficientnet'), \"EfficientNet branch\"),\n    (\"Morpho Branch\", hasattr(model, 'morpho_mlp'), \"MorphoMLP branch\"),\n    (\"Fusion Layer\", hasattr(model, 'fusion'), \"Concatenation fusion\"),\n]\nfor name, status, detail in checks:\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {name}: {detail}\")\n    checklist.append({'category': 'Model', 'check': name, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 4: Training & Evaluation\nprint(\"\\n📋 CATEGORY 4: TRAINING & EVALUATION\")\nchecks = [\n    (\"Training Completed\", os.path.exists('logs/training_history.json'), \"Training history saved\"),\n    (\"Best Model Saved\", os.path.exists('models/best_model.pth'), \"Best checkpoint exists\"),\n    (\"Training Curves\", os.path.exists('figures/training_curves.png'), \"Loss/accuracy plots\"),\n    (\"Test Evaluation\", 'all_preds' in locals(), \"Test predictions computed\"),\n]\nfor name, status, detail in checks:\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {name}: {detail}\")\n    checklist.append({'category': 'Training', 'check': name, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 5: Visualizations\nprint(\"\\n📋 CATEGORY 5: VISUALIZATIONS\")\nrequired_figures = [\n    ('augmentation_examples.png', 'Augmentation showcase'),\n    ('segmentation_examples.png', 'Segmentation results'),\n    ('morphometric_examples.png', 'Morphometric features'),\n    ('architecture_diagram.png', 'Model architecture'),\n    ('training_curves.png', 'Training curves'),\n    ('confusion_matrix.png', 'Confusion matrix'),\n    ('sample_predictions.png', 'Sample predictions'),\n]\nfor filename, description in required_figures:\n    filepath = f'figures/{filename}'\n    status = os.path.exists(filepath)\n    size = f\"{os.path.getsize(filepath)/1024:.1f} KB\" if status else \"N/A\"\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {filename}: {description} ({size})\")\n    checklist.append({'category': 'Figures', 'check': filename, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 6: Data Tables\nprint(\"\\n📋 CATEGORY 6: DATA TABLES\")\nrequired_tables = [\n    ('classification_report.json', 'Per-class metrics'),\n    ('per_class_metrics.csv', 'Metrics table'),\n    ('confusion_matrix.csv', 'Confusion matrix data'),\n    ('causal_inference_results.csv', 'Causal analysis'),\n    ('sample_treatment_reports.txt', 'Treatment examples'),\n]\nfor filename, description in required_tables:\n    filepath = f'tables/{filename}'\n    status = os.path.exists(filepath)\n    size = f\"{os.path.getsize(filepath)/1024:.1f} KB\" if status else \"N/A\"\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {filename}: {description} ({size})\")\n    checklist.append({'category': 'Tables', 'check': filename, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 7: Causal Inference\nprint(\"\\n📋 CATEGORY 7: CAUSAL INFERENCE\")\nchecks = [\n    (\"Knowledge Base\", 'CAUSAL_RULES' in locals(), f\"{len(CAUSAL_RULES)} disease mappings\"),\n    (\"Causal Function\", 'causal_inference' in dir(), \"Inference function defined\"),\n    (\"Causal Results\", 'all_causal_results' in locals(), f\"{len(all_causal_results) if 'all_causal_results' in locals() else 0} predictions analyzed\"),\n]\nfor name, status, detail in checks:\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {name}: {detail}\")\n    checklist.append({'category': 'Causal', 'check': name, 'status': 'PASS' if status else 'FAIL'})\n\n# Category 8: Final Packaging\nprint(\"\\n📋 CATEGORY 8: FINAL PACKAGING\")\nchecks = [\n    (\"Appendix Generated\", os.path.exists('appendix.md'), f\"{os.path.getsize('appendix.md')/1024:.1f} KB\" if os.path.exists('appendix.md') else \"N/A\"),\n    (\"Results Archive\", os.path.exists('final_results.zip'), f\"{os.path.getsize('final_results.zip')/(1024*1024):.2f} MB\" if os.path.exists('final_results.zip') else \"N/A\"),\n    (\"Environment Info\", os.path.exists('environment.txt'), \"Package versions\"),\n    (\"Data Sources\", os.path.exists('sources.txt'), \"Dataset citations\"),\n]\nfor name, status, detail in checks:\n    symbol = \"✅ PASS\" if status else \"❌ FAIL\"\n    print(f\"  {symbol} | {name}: {detail}\")\n    checklist.append({'category': 'Packaging', 'check': name, 'status': 'PASS' if status else 'FAIL'})\n\n# Summary\nprint(\"\\n\" + \"=\"*70)\ntotal_checks = len(checklist)\npassed_checks = sum(1 for c in checklist if c['status'] == 'PASS')\nfailed_checks = total_checks - passed_checks\n\nprint(f\"📊 VERIFICATION SUMMARY\")\nprint(f\"  Total Checks: {total_checks}\")\nprint(f\"  Passed: {passed_checks} ✅\")\nprint(f\"  Failed: {failed_checks} ❌\")\nprint(f\"  Success Rate: {passed_checks/total_checks*100:.1f}%\")\n\nif failed_checks == 0:\n    print(\"\\n🎉 ALL CHECKS PASSED! Notebook is complete and ready for patent submission.\")\nelse:\n    print(f\"\\n⚠️  {failed_checks} check(s) failed. Review the checklist above.\")\n\nprint(\"=\"*70 + \"\\n\")\n\n# Save checklist\nchecklist_df = pd.DataFrame(checklist)\nchecklist_df.to_csv('tables/verification_checklist.csv', index=False)\nprint(\"✓ Verification checklist saved to tables/verification_checklist.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:07:47.668206Z","iopub.execute_input":"2025-11-05T05:07:47.66898Z","iopub.status.idle":"2025-11-05T05:07:47.691335Z","shell.execute_reply.started":"2025-11-05T05:07:47.668954Z","shell.execute_reply":"2025-11-05T05:07:47.690554Z"}},"outputs":[],"execution_count":null}]}